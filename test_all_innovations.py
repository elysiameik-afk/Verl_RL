#!/usr/bin/env python3
"""
æµ‹è¯•æ‰€æœ‰å…­ä¸ªåˆ›æ–°ç‚¹çš„å®ç°
"""

import torch
import numpy as np
from verl.trainer.ppo.core_algos import (
    apply_token_level_ema_smoothing,
    apply_gradient_adaptive_weighting,
    apply_amic_aggregation,
    apply_temporal_decay_weighting,
    apply_ptrw_objective,
    apply_asymmetric_clipping,
    compute_policy_loss_with_innovations
)

def test_innovation_2_1_ema():
    """æµ‹è¯•åˆ›æ–°ç‚¹2.1: æ—¶åºå¹³æ»‘ (EMA) çš„é‡è¦æ€§æƒé‡"""
    print("ğŸ¯ æµ‹è¯•åˆ›æ–°ç‚¹2.1: æ—¶åºå¹³æ»‘ (EMA)")
    
    batch_size, seq_len = 2, 5
    raw_weights = torch.tensor([
        [1.2, 0.8, 1.5, 0.9, 1.1],
        [0.7, 1.3, 0.6, 1.4, 1.0]
    ])
    response_mask = torch.tensor([
        [1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1]
    ])
    
    smoothed_weights, metrics = apply_token_level_ema_smoothing(
        raw_weights=raw_weights,
        response_mask=response_mask,
        beta=0.7
    )
    
    print(f"  åŸå§‹æƒé‡æ–¹å·®: {metrics['ema/raw_weights_variance']:.4f}")
    print(f"  å¹³æ»‘æƒé‡æ–¹å·®: {metrics['ema/smoothed_weights_variance']:.4f}")
    print(f"  æ–¹å·®é™ä½æ¯”ä¾‹: {metrics['ema/variance_reduction_ratio']:.4f}")
    print("  âœ… EMAæµ‹è¯•é€šè¿‡\n")

def test_innovation_2_2_gradient_adaptive():
    """æµ‹è¯•åˆ›æ–°ç‚¹2.2: æ¢¯åº¦è‡ªé€‚åº”é‡è¦æ€§åŠ æƒ"""
    print("ğŸ¯ æµ‹è¯•åˆ›æ–°ç‚¹2.2: æ¢¯åº¦è‡ªé€‚åº”é‡è¦æ€§åŠ æƒ")
    
    batch_size, seq_len = 2, 4
    log_probs = torch.tensor([
        [-0.5, -1.2, -0.8, -0.3],
        [-0.9, -0.4, -1.1, -0.6]
    ], requires_grad=False)
    response_mask = torch.tensor([
        [1, 1, 1, 1],
        [1, 1, 1, 0]
    ])
    
    contribution_weights, metrics = apply_gradient_adaptive_weighting(
        log_probs=log_probs,
        response_mask=response_mask,
        temperature=1.0
    )
    
    print(f"  å¹³å‡æ¢¯åº¦èŒƒæ•°: {metrics['gradient_adaptive/avg_gradient_norm']:.4f}")
    print(f"  æƒé‡æ–¹å·®: {metrics['gradient_adaptive/weight_variance']:.4f}")
    print(f"  æƒé‡å‡å€¼: {metrics['gradient_adaptive/weight_mean']:.4f}")
    print("  âœ… æ¢¯åº¦è‡ªé€‚åº”æµ‹è¯•é€šè¿‡\n")

def test_innovation_2_3_amic():
    """æµ‹è¯•åˆ›æ–°ç‚¹2.3: ç®—æœ¯å¹³å‡é‡è¦æ€§æ ¡æ­£ (AMIC)"""
    print("ğŸ¯ æµ‹è¯•åˆ›æ–°ç‚¹2.3: ç®—æœ¯å¹³å‡é‡è¦æ€§æ ¡æ­£ (AMIC)")
    
    batch_size, seq_len = 2, 4
    raw_weights = torch.tensor([
        [1.2, 0.8, 1.5, 0.9],
        [0.7, 1.3, 0.6, 1.4]
    ])
    response_mask = torch.tensor([
        [1, 1, 1, 1],
        [1, 1, 1, 0]
    ])
    
    sequence_weights, metrics = apply_amic_aggregation(
        raw_weights=raw_weights,
        response_mask=response_mask
    )
    
    print(f"  åºåˆ—æƒé‡å‡å€¼: {metrics['amic/sequence_weights_mean']:.4f}")
    print(f"  åºåˆ—æƒé‡æ–¹å·®: {metrics['amic/sequence_weights_variance']:.4f}")
    print(f"  å¹³å‡åºåˆ—é•¿åº¦: {metrics['amic/avg_sequence_length']:.1f}")
    print("  âœ… AMICæµ‹è¯•é€šè¿‡\n")

def test_innovation_2_4_ptrw():
    """æµ‹è¯•åˆ›æ–°ç‚¹2.4: æ¦‚ç‡æ€§ä¿¡ä»»åŒºåŸŸåŠ æƒ (PTRW)"""
    print("ğŸ¯ æµ‹è¯•åˆ›æ–°ç‚¹2.4: æ¦‚ç‡æ€§ä¿¡ä»»åŒºåŸŸåŠ æƒ (PTRW)")
    
    importance_weights = torch.tensor([1.1, 0.9, 1.3, 0.7])
    advantages = torch.tensor([0.5, -0.3, 0.8, -0.2])
    
    ptrw_loss, metrics = apply_ptrw_objective(
        importance_weights=importance_weights,
        advantages=advantages,
        sigma=0.2
    )
    
    print(f"  ä¿¡ä»»æƒé‡å‡å€¼: {metrics['ptrw/trust_weights_mean']:.4f}")
    print(f"  ä¿¡ä»»æƒé‡æ ‡å‡†å·®: {metrics['ptrw/trust_weights_std']:.4f}")
    print(f"  PTRWæŸå¤±å‡å€¼: {metrics['ptrw/loss_mean']:.4f}")
    print("  âœ… PTRWæµ‹è¯•é€šè¿‡\n")

def test_innovation_2_5_temporal_decay():
    """æµ‹è¯•åˆ›æ–°ç‚¹2.5: åŸºäºæ—¶åºè¡°å‡çš„ä¼˜åŠ¿å¡‘é€ """
    print("ğŸ¯ æµ‹è¯•åˆ›æ–°ç‚¹2.5: åŸºäºæ—¶åºè¡°å‡çš„ä¼˜åŠ¿å¡‘é€ ")
    
    sequence_length = 5
    gamma = 0.9
    
    decay_weights, metrics = apply_temporal_decay_weighting(
        sequence_length=sequence_length,
        gamma=gamma,
        normalize=True
    )
    
    print(f"  è¡°å‡å› å­: {metrics['temporal_decay/gamma']:.2f}")
    print(f"  æƒé‡æ€»å’Œ: {metrics['temporal_decay/weight_sum']:.4f}")
    print(f"  é¦–ä¸ªæƒé‡: {metrics['temporal_decay/first_weight']:.4f}")
    print(f"  æœ€åæƒé‡: {metrics['temporal_decay/last_weight']:.4f}")
    print("  âœ… æ—¶åºè¡°å‡æµ‹è¯•é€šè¿‡\n")

def test_innovation_2_6_asymmetric():
    """æµ‹è¯•åˆ›æ–°ç‚¹2.6: æ­£è´Ÿä¼˜åŠ¿çš„éå¯¹ç§°ç­–ç•¥ä¼˜åŒ–"""
    print("ğŸ¯ æµ‹è¯•åˆ›æ–°ç‚¹2.6: æ­£è´Ÿä¼˜åŠ¿çš„éå¯¹ç§°ç­–ç•¥ä¼˜åŒ–")
    
    importance_weights = torch.tensor([1.1, 0.9, 1.3, 0.7])
    advantages = torch.tensor([0.5, -0.3, 0.8, -0.2])
    
    clipped_weights, metrics = apply_asymmetric_clipping(
        importance_weights=importance_weights,
        advantages=advantages,
        clip_ratio_pos=0.3,
        clip_ratio_neg=0.1
    )
    
    print(f"  æ­£ä¼˜åŠ¿æ¯”ä¾‹: {metrics['asymmetric/pos_advantage_ratio']:.2f}")
    print(f"  è´Ÿä¼˜åŠ¿æ¯”ä¾‹: {metrics['asymmetric/neg_advantage_ratio']:.2f}")
    print(f"  æ­£æ ·æœ¬è£å‰ªæ¯”ä¾‹: {metrics['asymmetric/pos_clipped_ratio']:.2f}")
    print(f"  è´Ÿæ ·æœ¬è£å‰ªæ¯”ä¾‹: {metrics['asymmetric/neg_clipped_ratio']:.2f}")
    print("  âœ… éå¯¹ç§°è£å‰ªæµ‹è¯•é€šè¿‡\n")

def test_comprehensive_policy_loss():
    """æµ‹è¯•ç»¼åˆç­–ç•¥æŸå¤±å‡½æ•°"""
    print("ğŸ¯ æµ‹è¯•ç»¼åˆç­–ç•¥æŸå¤±å‡½æ•°")
    
    batch_size, seq_len = 2, 4
    old_log_prob = torch.tensor([
        [-0.5, -1.2, -0.8, -0.3],
        [-0.9, -0.4, -1.1, -0.6]
    ])
    log_prob = torch.tensor([
        [-0.4, -1.1, -0.9, -0.4],
        [-0.8, -0.5, -1.0, -0.7]
    ])
    advantages = torch.tensor([
        [0.5, -0.3, 0.8, -0.2],
        [-0.1, 0.6, -0.4, 0.3]
    ])
    response_mask = torch.tensor([
        [1, 1, 1, 1],
        [1, 1, 1, 0]
    ])
    
    # æµ‹è¯•æ‰€æœ‰åˆ›æ–°ç‚¹ç»„åˆ
    pg_loss, pg_clipfrac, ppo_kl, pg_clipfrac_lower, metrics = compute_policy_loss_with_innovations(
        old_log_prob=old_log_prob,
        log_prob=log_prob,
        advantages=advantages,
        response_mask=response_mask,
        cliprange=0.2,
        use_ema_smoothing=True,
        ema_beta=0.9,
        use_gradient_adaptive_weighting=True,
        gradient_weighting_temperature=1.0,
        use_amic=False,  # ä¸GSPOäº’æ–¥
        use_ptrw=False,  # ä¸æ ‡å‡†è£å‰ªäº’æ–¥
        use_temporal_decay=True,
        temporal_decay_gamma=0.95,
        use_asymmetric_clipping=False,  # ä¸PTRWäº’æ–¥
    )
    
    print(f"  ç­–ç•¥æŸå¤±: {pg_loss.item():.4f}")
    print(f"  è£å‰ªæ¯”ä¾‹: {pg_clipfrac.item():.4f}")
    print(f"  PPO KL: {ppo_kl.item():.4f}")
    print(f"  æœ€ç»ˆæƒé‡å‡å€¼: {metrics['innovation/final_ratio_mean']:.4f}")
    print(f"  æœ€ç»ˆæƒé‡æ ‡å‡†å·®: {metrics['innovation/final_ratio_std']:.4f}")
    print("  âœ… ç»¼åˆç­–ç•¥æŸå¤±æµ‹è¯•é€šè¿‡\n")

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æµ‹è¯•æ‰€æœ‰å…­ä¸ªåˆ›æ–°ç‚¹...\n")
    
    test_innovation_2_1_ema()
    test_innovation_2_2_gradient_adaptive()
    test_innovation_2_3_amic()
    test_innovation_2_4_ptrw()
    test_innovation_2_5_temporal_decay()
    test_innovation_2_6_asymmetric()
    test_comprehensive_policy_loss()
    
    print("ğŸ‰ æ‰€æœ‰åˆ›æ–°ç‚¹æµ‹è¯•å®Œæˆï¼")
    print("\nğŸ“‹ åˆ›æ–°ç‚¹æ€»ç»“:")
    print("  2.1 âœ… æ—¶åºå¹³æ»‘ (EMA) çš„é‡è¦æ€§æƒé‡")
    print("  2.2 âœ… æ¢¯åº¦è‡ªé€‚åº”é‡è¦æ€§åŠ æƒ")
    print("  2.3 âœ… ç®—æœ¯å¹³å‡é‡è¦æ€§æ ¡æ­£ (AMIC)")
    print("  2.4 âœ… æ¦‚ç‡æ€§ä¿¡ä»»åŒºåŸŸåŠ æƒ (PTRW)")
    print("  2.5 âœ… åŸºäºæ—¶åºè¡°å‡çš„ä¼˜åŠ¿å¡‘é€ ")
    print("  2.6 âœ… æ­£è´Ÿä¼˜åŠ¿çš„éå¯¹ç§°ç­–ç•¥ä¼˜åŒ–")
    print("\nğŸ¯ æ‰€æœ‰åˆ›æ–°ç‚¹å·²å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥å¼€å§‹å®éªŒï¼")
