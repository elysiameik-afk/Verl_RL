#!/usr/bin/env python3
"""
æµ‹è¯•EMAå¹³æ»‘å®ç°çš„è„šæœ¬
"""

import torch
import numpy as np
from verl.trainer.ppo.core_algos import apply_ema_smoothing, compute_policy_loss_with_ema

def test_ema_smoothing():
    """æµ‹è¯•EMAå¹³æ»‘åŠŸèƒ½"""
    print("=== æµ‹è¯•EMAå¹³æ»‘åŠŸèƒ½ ===")
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    batch_size, seq_len = 4, 10
    raw_weights = torch.randn(batch_size, seq_len) * 2.0 + 1.0  # æ¨¡æ‹Ÿé‡è¦æ€§æƒé‡
    raw_weights = torch.exp(raw_weights)  # ç¡®ä¿ä¸ºæ­£æ•°
    response_mask = torch.ones(batch_size, seq_len)
    sequence_ids = ['seq_0', 'seq_1', 'seq_2', 'seq_3']
    beta = 0.9
    
    # åˆå§‹åŒ–EMAçŠ¶æ€
    ema_weights_state = {}
    
    print(f"åŸå§‹æƒé‡æ–¹å·®: {(raw_weights * response_mask).var().item():.6f}")
    print(f"åŸå§‹æƒé‡å‡å€¼: {(raw_weights * response_mask).mean().item():.6f}")
    
    # åº”ç”¨EMAå¹³æ»‘
    smoothed_weights, ema_metrics = apply_ema_smoothing(
        raw_weights=raw_weights,
        ema_weights_state=ema_weights_state,
        sequence_ids=sequence_ids,
        response_mask=response_mask,
        beta=beta,
    )
    
    print(f"å¹³æ»‘åæƒé‡æ–¹å·®: {ema_metrics['ema/smoothed_weights_variance']:.6f}")
    print(f"å¹³æ»‘åæƒé‡å‡å€¼: {ema_metrics['ema/smoothed_weights_mean']:.6f}")
    print(f"æ–¹å·®é™ä½æ¯”ä¾‹: {ema_metrics['ema/variance_reduction_ratio']:.6f}")
    print(f"å¹³æ»‘å¼ºåº¦: {ema_metrics['ema/avg_sequence_diff_l2']:.6f}")
    
    # æµ‹è¯•å¤šæ­¥EMA
    print("\n=== æµ‹è¯•å¤šæ­¥EMA ===")
    for step in range(3):
        new_raw_weights = torch.randn(batch_size, seq_len) * 1.5 + 1.0
        new_raw_weights = torch.exp(new_raw_weights)
        
        smoothed_weights, ema_metrics = apply_ema_smoothing(
            raw_weights=new_raw_weights,
            ema_weights_state=ema_weights_state,
            sequence_ids=sequence_ids,
            response_mask=response_mask,
            beta=beta,
        )
        
        print(f"æ­¥éª¤ {step+1}: æ–¹å·®é™ä½æ¯”ä¾‹ = {ema_metrics['ema/variance_reduction_ratio']:.6f}")
    
    return True

def test_policy_loss_with_ema():
    """æµ‹è¯•å¸¦EMAçš„ç­–ç•¥æŸå¤±è®¡ç®—"""
    print("\n=== æµ‹è¯•ç­–ç•¥æŸå¤±è®¡ç®— ===")
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    batch_size, seq_len = 2, 8
    old_log_prob = torch.randn(batch_size, seq_len) * 0.1
    log_prob = old_log_prob + torch.randn(batch_size, seq_len) * 0.05  # ç¨å¾®ä¸åŒ
    advantages = torch.randn(batch_size, seq_len) * 0.5
    response_mask = torch.ones(batch_size, seq_len)
    sequence_ids = ['seq_A', 'seq_B']
    
    # åˆå§‹åŒ–EMAçŠ¶æ€
    ema_weights_state = {}
    
    # æµ‹è¯•ä¸ä½¿ç”¨EMA
    pg_loss_no_ema, pg_clipfrac_no_ema, ppo_kl_no_ema, pg_clipfrac_lower_no_ema, ema_metrics_no_ema = compute_policy_loss_with_ema(
        old_log_prob=old_log_prob,
        log_prob=log_prob,
        advantages=advantages,
        response_mask=response_mask,
        cliprange=0.2,
        use_ema=False,
    )
    
    print(f"ä¸ä½¿ç”¨EMA - ç­–ç•¥æŸå¤±: {pg_loss_no_ema.item():.6f}")
    print(f"ä¸ä½¿ç”¨EMA - åŸå§‹æƒé‡æ–¹å·®: {ema_metrics_no_ema['ema/raw_weights_variance']:.6f}")
    
    # æµ‹è¯•ä½¿ç”¨EMA
    pg_loss_ema, pg_clipfrac_ema, ppo_kl_ema, pg_clipfrac_lower_ema, ema_metrics_ema = compute_policy_loss_with_ema(
        old_log_prob=old_log_prob,
        log_prob=log_prob,
        advantages=advantages,
        response_mask=response_mask,
        cliprange=0.2,
        ema_weights_state=ema_weights_state,
        sequence_ids=sequence_ids,
        beta=0.9,
        use_ema=True,
    )
    
    print(f"ä½¿ç”¨EMA - ç­–ç•¥æŸå¤±: {pg_loss_ema.item():.6f}")
    print(f"ä½¿ç”¨EMA - åŸå§‹æƒé‡æ–¹å·®: {ema_metrics_ema['ema/raw_weights_variance']:.6f}")
    print(f"ä½¿ç”¨EMA - å¹³æ»‘æƒé‡æ–¹å·®: {ema_metrics_ema['ema/smoothed_weights_variance']:.6f}")
    print(f"ä½¿ç”¨EMA - æ–¹å·®é™ä½æ¯”ä¾‹: {ema_metrics_ema['ema/variance_reduction_ratio']:.6f}")
    
    return True

def test_different_beta_values():
    """æµ‹è¯•ä¸åŒÎ²å€¼çš„æ•ˆæœ"""
    print("\n=== æµ‹è¯•ä¸åŒÎ²å€¼çš„æ•ˆæœ ===")
    
    batch_size, seq_len = 3, 6
    raw_weights = torch.randn(batch_size, seq_len) * 3.0 + 1.0
    raw_weights = torch.exp(raw_weights)
    response_mask = torch.ones(batch_size, seq_len)
    sequence_ids = ['seq_1', 'seq_2', 'seq_3']
    
    beta_values = [0.5, 0.7, 0.9, 0.95, 0.99]
    
    for beta in beta_values:
        ema_weights_state = {}
        smoothed_weights, ema_metrics = apply_ema_smoothing(
            raw_weights=raw_weights,
            ema_weights_state=ema_weights_state,
            sequence_ids=sequence_ids,
            response_mask=response_mask,
            beta=beta,
        )
        
        print(f"Î²={beta:.2f}: æ–¹å·®é™ä½æ¯”ä¾‹={ema_metrics['ema/variance_reduction_ratio']:.4f}, "
              f"å¹³æ»‘å¼ºåº¦={ema_metrics['ema/avg_sequence_diff_l2']:.4f}")
    
    return True

if __name__ == "__main__":
    print("å¼€å§‹æµ‹è¯•EMAå®ç°...")
    
    try:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        test_ema_smoothing()
        test_policy_loss_with_ema()
        test_different_beta_values()
        
        print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼EMAå®ç°æ­£å¸¸å·¥ä½œã€‚")
        print("\nğŸš€ ç°åœ¨ä½ å¯ä»¥è¿è¡Œè®­ç»ƒè„šæœ¬ï¼š")
        print("   bash exp/qwen2.5kk1_ema.sh")
        print("\nğŸ“Š WandBå°†è®°å½•ä»¥ä¸‹å…³é”®æŒ‡æ ‡ï¼š")
        print("   - ema/raw_weights_variance: åŸå§‹æƒé‡æ–¹å·®")
        print("   - ema/smoothed_weights_variance: å¹³æ»‘æƒé‡æ–¹å·®") 
        print("   - ema/variance_reduction_ratio: æ–¹å·®é™ä½æ¯”ä¾‹")
        print("   - ema/smoothing_strength: å¹³æ»‘å¼ºåº¦")
        print("   - ema/range_reduction: æƒé‡èŒƒå›´æ”¶ç¼©")
        print("   - ä»¥åŠæ›´å¤šè¯¦ç»†åˆ†ææŒ‡æ ‡...")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
