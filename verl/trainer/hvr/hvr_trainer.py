"""
HVRè®­ç»ƒå™¨å®ç°

åŸºäºRayçš„åˆ†å¸ƒå¼HVRè®­ç»ƒå™¨ï¼Œä¸“é—¨ä¸ºå†…ç”Ÿå¥–åŠ±æœºåˆ¶è®¾è®¡ï¼š
1. ç®€åŒ–çš„è®­ç»ƒæµç¨‹ï¼ˆæ— criticï¼‰
2. ç›´æ¥ä½¿ç”¨HVRå¥–åŠ±è¿›è¡Œç­–ç•¥ä¼˜åŒ–
3. å¤ç”¨ç°æœ‰çš„rolloutå’Œå¥–åŠ±è®¡ç®—ç»„ä»¶
"""

import ray
import torch
from typing import Dict, Any
from omegaconf import DictConfig

from verl.trainer.ppo.ray_trainer import RayPPOTrainer
from verl.trainer.hvr.hvr_actor import HVRActor
from verl.trainer.ppo.core_algos import is_main_process
from verl.single_controller.ray.base import RayClassWithInitArgs


class HVRTrainer(RayPPOTrainer):
    """
    HVRä¸“ç”¨è®­ç»ƒå™¨
    
    ç»§æ‰¿RayPPOTrainerä½†ä¸“é—¨ä¸ºHVRå†…ç”Ÿå¥–åŠ±æœºåˆ¶è®¾è®¡ï¼š
    - ç§»é™¤criticç›¸å…³é€»è¾‘
    - ä½¿ç”¨HVRActoræ›¿ä»£æ ‡å‡†Actor
    - ç®€åŒ–çš„è®­ç»ƒå¾ªç¯
    """
    
    def __init__(self, config: DictConfig):
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–ï¼Œä½†ä¼šåœ¨åç»­æ›¿æ¢actor
        super().__init__(config)
        
        if is_main_process():
            print("ğŸ¯ [HVR Trainer] åˆå§‹åŒ–HVRè®­ç»ƒå™¨")
            print("ğŸ¯ [HVRç‰¹æ€§] å†…ç”Ÿå¥–åŠ±æœºåˆ¶ï¼Œæ— éœ€criticç½‘ç»œ")
    
    def init_workers(self):
        """åˆå§‹åŒ–HVRä¸“ç”¨çš„workers"""
        if is_main_process():
            print("ğŸ¯ [HVR] åˆå§‹åŒ–HVRä¸“ç”¨workers...")
        
        # åˆå§‹åŒ–rollout workers (å¤ç”¨ç°æœ‰å®ç°)
        self.rollout_wg.init_model()
        
        # åˆå§‹åŒ–reference policy (å¤ç”¨ç°æœ‰å®ç°)  
        self.ref_policy_wg.init_model()
        
        # åˆå§‹åŒ–HVR Actor (æ›¿æ¢æ ‡å‡†actor)
        self._init_hvr_actor()
        
        if is_main_process():
            print("âœ… [HVR] HVR workersåˆå§‹åŒ–å®Œæˆ")
    
    def _init_hvr_actor(self):
        """åˆå§‹åŒ–HVRä¸“ç”¨Actor"""
        if is_main_process():
            print("ğŸ¯ [HVR] åˆå§‹åŒ–HVR Actor...")
        
        # åˆ›å»ºHVR Actoré…ç½®
        hvr_actor_config = self.config.actor_rollout_ref.actor.copy()
        
        # ç¡®ä¿HVRå¿…éœ€çš„é…ç½®
        hvr_actor_config.use_remove_padding = False  # HVRéœ€è¦å®Œæ•´logits
        
        # åˆ›å»ºHVR Actorç±»
        @ray.remote(num_gpus=self.config.trainer.n_gpus_per_node)
        class HVRActorWorker(RayClassWithInitArgs):
            def __init__(self, config):
                self.actor = HVRActor(config)
            
            def update_policy(self, data):
                return self.actor.update_policy(data)
            
            def generate_sequences(self, prompts, **kwargs):
                return self.actor.generate_sequences(prompts, **kwargs)
        
        # åˆå§‹åŒ–HVR Actor workers
        self.hvr_actor_wg = HVRActorWorker.remote(hvr_actor_config)
        
        if is_main_process():
            print("âœ… [HVR] HVR Actoråˆå§‹åŒ–å®Œæˆ")
    
    def fit(self):
        """HVRä¸“ç”¨çš„è®­ç»ƒå¾ªç¯"""
        if is_main_process():
            print("ğŸš€ [HVR] å¼€å§‹HVRè®­ç»ƒå¾ªç¯")
        
        for epoch in range(self.config.trainer.total_epochs):
            if is_main_process():
                print(f"\nğŸ¯ [HVR] Epoch {epoch + 1}/{self.config.trainer.total_epochs}")
            
            # è®­ç»ƒä¸€ä¸ªepoch
            epoch_metrics = self._train_epoch(epoch)
            
            # è®°å½•æŒ‡æ ‡
            if self.logger:
                self.logger.log(epoch_metrics, step=epoch)
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % self.config.trainer.save_freq == 0:
                self._save_checkpoint(epoch)
            
            # éªŒè¯
            if (epoch + 1) % self.config.trainer.test_freq == 0:
                val_metrics = self._validate(epoch)
                if self.logger:
                    self.logger.log(val_metrics, step=epoch)
        
        if is_main_process():
            print("ğŸ‰ [HVR] HVRè®­ç»ƒå®Œæˆï¼")
    
    def _train_epoch(self, epoch: int) -> Dict[str, Any]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        epoch_metrics = {}
        
        # è·å–è®­ç»ƒæ•°æ®
        for batch_idx, batch in enumerate(self.train_dataloader):
            if is_main_process() and batch_idx % 10 == 0:
                print(f"ğŸ”„ [HVR] Epoch {epoch}, Batch {batch_idx}")
            
            # æ‰§è¡Œä¸€ä¸ªè®­ç»ƒæ­¥éª¤
            step_metrics = self._train_step(batch)
            
            # èšåˆæŒ‡æ ‡
            for key, value in step_metrics.items():
                if key not in epoch_metrics:
                    epoch_metrics[key] = []
                epoch_metrics[key].append(value)
        
        # è®¡ç®—epochå¹³å‡æŒ‡æ ‡
        for key, values in epoch_metrics.items():
            if isinstance(values[0], (int, float)):
                epoch_metrics[key] = sum(values) / len(values)
        
        return epoch_metrics
    
    def _train_step(self, batch) -> Dict[str, Any]:
        """æ‰§è¡Œä¸€ä¸ªè®­ç»ƒæ­¥éª¤"""
        # 1. Rollouté˜¶æ®µ (å¤ç”¨ç°æœ‰å®ç°)
        rollout_output = self.rollout_wg.rollout(batch)
        
        # 2. å¥–åŠ±è®¡ç®— (å¤ç”¨ç°æœ‰çš„logic_rlç­‰)
        reward_output = self.reward_wg.compute_reward(rollout_output)
        
        # 3. Reference policyè®¡ç®— (å¤ç”¨ç°æœ‰å®ç°)
        ref_output = self.ref_policy_wg.compute_ref_log_prob(rollout_output)
        
        # 4. å‡†å¤‡HVRè®­ç»ƒæ•°æ®
        hvr_batch = self._prepare_hvr_batch(rollout_output, reward_output, ref_output)
        
        # 5. HVR Actoræ›´æ–°
        actor_metrics = ray.get(self.hvr_actor_wg.update_policy.remote(hvr_batch))
        
        return actor_metrics
    
    def _prepare_hvr_batch(self, rollout_output, reward_output, ref_output):
        """å‡†å¤‡HVRè®­ç»ƒæ•°æ®"""
        # ç»„åˆæ‰€æœ‰å¿…éœ€çš„æ•°æ®
        hvr_batch = {
            # åŸºç¡€åºåˆ—æ•°æ®
            "responses": rollout_output["responses"],
            "input_ids": rollout_output["input_ids"], 
            "attention_mask": rollout_output["attention_mask"],
            "position_ids": rollout_output["position_ids"],
            
            # å‚è€ƒç­–ç•¥æ•°æ®
            "ref_log_prob": ref_output["ref_log_prob"],
            
            # å…¶ä»–å…ƒæ•°æ®
            "uid": rollout_output.get("uid", []),
        }
        
        # æ·»åŠ å¥–åŠ±æ•°æ®åˆ°batchä¸­
        if hasattr(reward_output, 'batch'):
            hvr_batch.batch = reward_output.batch
        else:
            # å¦‚æœreward_outputä¸æ˜¯batchæ ¼å¼ï¼Œåˆ›å»ºbatch
            hvr_batch.batch = {
                "token_level_rewards": reward_output.get("token_level_rewards"),
                "token_level_scores": reward_output.get("token_level_scores"),
            }
        
        return hvr_batch
    
    def _validate(self, epoch: int) -> Dict[str, Any]:
        """éªŒè¯é˜¶æ®µ"""
        if is_main_process():
            print(f"ğŸ” [HVR] æ‰§è¡ŒéªŒè¯ (Epoch {epoch})")
        
        # ç®€åŒ–çš„éªŒè¯é€»è¾‘
        val_metrics = {
            "val/epoch": epoch,
            "val/hvr_validation": True,
        }
        
        return val_metrics
    
    def _save_checkpoint(self, epoch: int):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        if is_main_process():
            print(f"ğŸ’¾ [HVR] ä¿å­˜æ£€æŸ¥ç‚¹ (Epoch {epoch})")
        
        # ä¿å­˜HVR ActorçŠ¶æ€
        # è¿™é‡Œå¯ä»¥æ·»åŠ å…·ä½“çš„ä¿å­˜é€»è¾‘
        pass
    
    @property
    def train_dataloader(self):
        """è®­ç»ƒæ•°æ®åŠ è½½å™¨ (å¤ç”¨çˆ¶ç±»å®ç°)"""
        return super().train_dataloader
    
    @property 
    def val_dataloader(self):
        """éªŒè¯æ•°æ®åŠ è½½å™¨ (å¤ç”¨çˆ¶ç±»å®ç°)"""
        return super().val_dataloader
