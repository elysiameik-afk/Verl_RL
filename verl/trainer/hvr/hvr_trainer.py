"""
HVRè®­ç»ƒå™¨å®ç°

åŸºäºRayçš„åˆ†å¸ƒå¼HVRè®­ç»ƒå™¨ï¼Œä¸“é—¨ä¸ºå†…ç”Ÿå¥–åŠ±æœºåˆ¶è®¾è®¡ï¼š
1. ç®€åŒ–çš„è®­ç»ƒæµç¨‹ï¼ˆæ— criticï¼‰
2. ç›´æ¥ä½¿ç”¨HVRå¥–åŠ±è¿›è¡Œç­–ç•¥ä¼˜åŒ–
3. å¤ç”¨ç°æœ‰çš„rolloutå’Œå¥–åŠ±è®¡ç®—ç»„ä»¶
"""

import ray
import torch
import numpy as np
from typing import Dict, Any
from omegaconf import DictConfig

from verl.trainer.ppo.core_algos import is_main_process
from verl.single_controller.ray.base import RayClassWithInitArgs
from verl.trainer.hvr.hvr_actor import HVRActor


class HVRTrainer:
    """
    HVRä¸“ç”¨è®­ç»ƒå™¨

    ç‹¬ç«‹çš„HVRè®­ç»ƒå™¨ï¼Œä¸“é—¨ä¸ºå†…ç”Ÿå¥–åŠ±æœºåˆ¶è®¾è®¡ï¼š
    - ç§»é™¤criticç›¸å…³é€»è¾‘
    - ä½¿ç”¨HVRActoræ›¿ä»£æ ‡å‡†Actor
    - ç®€åŒ–çš„è®­ç»ƒå¾ªç¯
    """

    def __init__(self, config: DictConfig):
        self.config = config

        if is_main_process():
            print("ğŸ¯ [HVR Trainer] åˆå§‹åŒ–HVRè®­ç»ƒå™¨")
            print("ğŸ¯ [HVRç‰¹æ€§] å†…ç”Ÿå¥–åŠ±æœºåˆ¶ï¼Œæ— éœ€criticç½‘ç»œ")

        # åˆå§‹åŒ–ç»„ä»¶
        self.hvr_actor_wg = None
        self.rollout_wg = None
        self.ref_policy_wg = None
        self.reward_wg = None

        # åˆå§‹åŒ–WandBæ—¥å¿—è®°å½•å™¨
        self.logger = None
        self._init_logger()

        # è®­ç»ƒçŠ¶æ€
        self.global_step = 0
        self.epoch_metrics_history = []
    
    def init_workers(self):
        """åˆå§‹åŒ–HVRä¸“ç”¨çš„workers"""
        if is_main_process():
            print("ğŸ¯ [HVR] åˆå§‹åŒ–HVRä¸“ç”¨workers...")

        # åˆå§‹åŒ–HVR Actor
        self._init_hvr_actor()

        # åˆå§‹åŒ–å…¶ä»–å¿…éœ€çš„workers
        self._init_rollout_workers()
        self._init_ref_policy_workers()
        self._init_reward_workers()

        if is_main_process():
            print("âœ… [HVR] HVR workersåˆå§‹åŒ–å®Œæˆ")
    
    def _init_hvr_actor(self):
        """åˆå§‹åŒ–HVRä¸“ç”¨Actor"""
        if is_main_process():
            print("ğŸ¯ [HVR] åˆå§‹åŒ–HVR Actor...")
        
        # åˆ›å»ºHVR Actoré…ç½®
        hvr_actor_config = self.config.actor_rollout_ref.actor.copy()
        
        # ç¡®ä¿HVRå¿…éœ€çš„é…ç½®
        hvr_actor_config.use_remove_padding = False  # HVRéœ€è¦å®Œæ•´logits
        
        # åˆ›å»ºHVR Actorç±»
        @ray.remote(num_gpus=self.config.trainer.n_gpus_per_node)
        class HVRActorWorker(RayClassWithInitArgs):
            def __init__(self, config):
                self.actor = HVRActor(config)
            
            def update_policy(self, data):
                return self.actor.update_policy(data)
            
            def generate_sequences(self, prompts, **kwargs):
                return self.actor.generate_sequences(prompts, **kwargs)
        
        # åˆå§‹åŒ–HVR Actor workers
        self.hvr_actor_wg = HVRActorWorker.remote(hvr_actor_config)
        
        if is_main_process():
            print("âœ… [HVR] HVR Actoråˆå§‹åŒ–å®Œæˆ")

    def _init_rollout_workers(self):
        """åˆå§‹åŒ–rollout workers (ç®€åŒ–ç‰ˆæœ¬)"""
        if is_main_process():
            print("ğŸ¯ [HVR] åˆå§‹åŒ–Rollout workers...")

        # è¿™é‡Œåº”è¯¥åˆå§‹åŒ–rollout workers
        # æš‚æ—¶ä½¿ç”¨å ä½ç¬¦ï¼Œå®é™…éœ€è¦æ ¹æ®å…·ä½“éœ€æ±‚å®ç°
        self.rollout_wg = None

        if is_main_process():
            print("âš ï¸ [HVR] Rollout workersæš‚æœªå®ç°")

    def _init_ref_policy_workers(self):
        """åˆå§‹åŒ–reference policy workers"""
        if is_main_process():
            print("ğŸ¯ [HVR] åˆå§‹åŒ–Reference Policy workers...")

        self.ref_policy_wg = None

        if is_main_process():
            print("âš ï¸ [HVR] Reference Policy workersæš‚æœªå®ç°")

    def _init_reward_workers(self):
        """åˆå§‹åŒ–reward workers"""
        if is_main_process():
            print("ğŸ¯ [HVR] åˆå§‹åŒ–Reward workers...")

        self.reward_wg = None

        if is_main_process():
            print("âš ï¸ [HVR] Reward workersæš‚æœªå®ç°")

    def _init_logger(self):
        """åˆå§‹åŒ–WandBæ—¥å¿—è®°å½•å™¨"""
        if is_main_process() and "wandb" in self.config.trainer.get("logger", []):
            try:
                import wandb

                wandb.init(
                    project=self.config.trainer.project_name,
                    name=self.config.trainer.experiment_name,
                    config=dict(self.config),
                    tags=["HVR", "IntrinsicReward", "ERVF"],
                )

                self.logger = wandb
                print("âœ… [HVR] WandBæ—¥å¿—è®°å½•å™¨åˆå§‹åŒ–å®Œæˆ")

            except ImportError:
                print("âš ï¸ [HVR] WandBæœªå®‰è£…ï¼Œè·³è¿‡æ—¥å¿—è®°å½•")
                self.logger = None
        else:
            self.logger = None
    
    def fit(self):
        """HVRä¸“ç”¨çš„è®­ç»ƒå¾ªç¯"""
        if is_main_process():
            print("ğŸš€ [HVR] å¼€å§‹HVRè®­ç»ƒå¾ªç¯")

        # ç®€åŒ–çš„è®­ç»ƒå¾ªç¯ï¼Œä¸“æ³¨äºHVRç®—æ³•éªŒè¯
        for epoch in range(self.config.trainer.total_epochs):
            if is_main_process():
                print(f"\nğŸ¯ [HVR] Epoch {epoch + 1}/{self.config.trainer.total_epochs}")

            # ç®€åŒ–çš„è®­ç»ƒæ­¥éª¤
            try:
                epoch_metrics = self._train_epoch_simple(epoch)

                # è®°å½•åˆ°WandB
                if self.logger and is_main_process():
                    self.logger.log(epoch_metrics, step=epoch)

                # ä¿å­˜æŒ‡æ ‡å†å²
                self.epoch_metrics_history.append(epoch_metrics)

                if is_main_process():
                    print(f"ğŸ“Š [HVR] Epoch {epoch + 1} æŒ‡æ ‡: {epoch_metrics}")

            except Exception as e:
                if is_main_process():
                    print(f"âš ï¸ [HVR] Epoch {epoch + 1} è®­ç»ƒå‡ºé”™: {e}")
                continue

        if is_main_process():
            print("ğŸ‰ [HVR] HVRè®­ç»ƒå®Œæˆï¼")

    def _train_epoch_simple(self, epoch: int) -> Dict[str, Any]:
        """ç®€åŒ–çš„è®­ç»ƒepochï¼Œä¸“æ³¨äºHVRç®—æ³•éªŒè¯"""
        if is_main_process():
            print(f"ğŸ”„ [HVR] æ‰§è¡Œç®€åŒ–è®­ç»ƒ Epoch {epoch}")

        # æ¨¡æ‹Ÿå¤šä¸ªbatchçš„è®­ç»ƒ
        epoch_metrics = []
        num_batches = 5  # æ¨¡æ‹Ÿ5ä¸ªbatch

        for batch_idx in range(num_batches):
            batch_metrics = self._train_batch_hvr(epoch, batch_idx)
            epoch_metrics.append(batch_metrics)
            self.global_step += 1

        # èšåˆepochæŒ‡æ ‡
        aggregated_metrics = self._aggregate_batch_metrics(epoch_metrics)
        aggregated_metrics["epoch"] = epoch
        aggregated_metrics["global_step"] = self.global_step

        return aggregated_metrics

    def _train_batch_hvr(self, epoch: int, batch_idx: int) -> Dict[str, Any]:
        """è®­ç»ƒå•ä¸ªbatchï¼Œä½¿ç”¨HVRç®—æ³•"""
        try:
            # å¯¼å…¥HVRç®—æ³•
            from verl.trainer.hvr.hvr_core_algos import calculate_hvr_rewards

            # æ¨¡æ‹Ÿbatchæ•°æ®
            batch_size = 4
            seq_len = np.random.randint(8, 20)  # éšæœºåºåˆ—é•¿åº¦
            vocab_size = 1000

            # æ¨¡æ‹Ÿä¸åŒçš„logic_rlå¥–åŠ±
            possible_rewards = [-3, -1, 0, 1, 3]

            batch_metrics = []

            for i in range(batch_size):
                # åˆ›å»ºå•ä¸ªåºåˆ—çš„æµ‹è¯•æ•°æ®
                response_logits = torch.randn(seq_len, vocab_size)
                response_ids = torch.randint(0, vocab_size, (seq_len,))
                r_final = np.random.choice(possible_rewards)  # éšæœºé€‰æ‹©å¥–åŠ±

                # è®¡ç®—HVRå¥–åŠ±
                hvr_rewards, hvr_metrics = calculate_hvr_rewards(
                    response_logits=response_logits,
                    response_ids=response_ids,
                    R_final=r_final,
                    alpha=self.config.actor_rollout_ref.actor.hvr_alpha,
                    beta=self.config.actor_rollout_ref.actor.hvr_beta,
                    lambda_hvr=self.config.actor_rollout_ref.actor.hvr_lambda,
                )

                # æ¨¡æ‹Ÿç­–ç•¥æŸå¤±è®¡ç®—
                log_probs = torch.randn(seq_len)
                response_mask = torch.ones(seq_len)

                # è®¡ç®—ç­–ç•¥æŸå¤± (ç®€åŒ–ç‰ˆ)
                policy_loss = -(log_probs * hvr_rewards * response_mask).mean()

                # æ”¶é›†åºåˆ—æŒ‡æ ‡
                sequence_metrics = {
                    "r_final": r_final,
                    "hvr_reward_mean": hvr_rewards.mean().item(),
                    "hvr_reward_std": hvr_rewards.std().item(),
                    "ervf_value_mean": hvr_metrics.ervf_value_mean,
                    "entropy_mean": hvr_metrics.entropy_mean,
                    "policy_loss": policy_loss.item(),
                    "sequence_length": seq_len,
                }

                batch_metrics.append(sequence_metrics)

            # èšåˆbatchæŒ‡æ ‡
            aggregated = self._aggregate_sequence_metrics(batch_metrics)
            aggregated.update({
                "batch_idx": batch_idx,
                "epoch": epoch,
                "batch_size": batch_size,
                "hvr_success": True,
            })

            if is_main_process() and batch_idx % 2 == 0:
                print(f"   Batch {batch_idx}: HVRå¥–åŠ±å‡å€¼={aggregated['hvr/reward_mean']:.4f}, "
                      f"ç­–ç•¥æŸå¤±={aggregated['train/policy_loss']:.4f}")

            return aggregated

        except Exception as e:
            if is_main_process():
                print(f"âŒ [HVR] Batch {batch_idx} å¤±è´¥: {e}")

            return {
                "batch_idx": batch_idx,
                "epoch": epoch,
                "hvr_success": False,
                "error": str(e),
            }

    def _aggregate_sequence_metrics(self, sequence_metrics: list) -> Dict[str, Any]:
        """èšåˆåºåˆ—æŒ‡æ ‡"""
        if not sequence_metrics:
            return {}

        # æ•°å€¼æŒ‡æ ‡èšåˆ
        aggregated = {}

        # HVRç›¸å…³æŒ‡æ ‡
        aggregated["hvr/reward_mean"] = np.mean([m["hvr_reward_mean"] for m in sequence_metrics])
        aggregated["hvr/reward_std"] = np.mean([m["hvr_reward_std"] for m in sequence_metrics])
        aggregated["hvr/ervf_value_mean"] = np.mean([m["ervf_value_mean"] for m in sequence_metrics])
        aggregated["hvr/entropy_mean"] = np.mean([m["entropy_mean"] for m in sequence_metrics])

        # è®­ç»ƒæŒ‡æ ‡
        aggregated["train/policy_loss"] = np.mean([m["policy_loss"] for m in sequence_metrics])
        aggregated["train/sequence_length_mean"] = np.mean([m["sequence_length"] for m in sequence_metrics])

        # å¥–åŠ±åˆ†å¸ƒç»Ÿè®¡
        r_finals = [m["r_final"] for m in sequence_metrics]
        unique_rewards, counts = np.unique(r_finals, return_counts=True)
        for reward, count in zip(unique_rewards, counts):
            aggregated[f"reward_dist/r_{reward}"] = count

        aggregated["reward_dist/mean"] = np.mean(r_finals)
        aggregated["reward_dist/std"] = np.std(r_finals)

        return aggregated

    def _aggregate_batch_metrics(self, batch_metrics: list) -> Dict[str, Any]:
        """èšåˆå¤šä¸ªbatchçš„æŒ‡æ ‡"""
        if not batch_metrics:
            return {}

        # è¿‡æ»¤æˆåŠŸçš„batch
        successful_batches = [m for m in batch_metrics if m.get("hvr_success", False)]

        if not successful_batches:
            return {"hvr_epoch_success": False}

        # èšåˆæ‰€æœ‰æ•°å€¼æŒ‡æ ‡
        aggregated = {}

        for key in successful_batches[0].keys():
            if key.startswith(("hvr/", "train/", "reward_dist/")) and isinstance(successful_batches[0][key], (int, float)):
                values = [batch[key] for batch in successful_batches if key in batch]
                if values:
                    aggregated[key] = np.mean(values)

        # æ·»åŠ epochçº§åˆ«çš„æŒ‡æ ‡
        aggregated["hvr_epoch_success"] = True
        aggregated["successful_batches"] = len(successful_batches)
        aggregated["total_batches"] = len(batch_metrics)
        aggregated["success_rate"] = len(successful_batches) / len(batch_metrics)

        return aggregated
    
    # ç§»é™¤å¤æ‚çš„è®­ç»ƒæ–¹æ³•ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
    
    # ç®€åŒ–ç‰ˆæœ¬ï¼Œç§»é™¤å¤æ‚çš„éªŒè¯å’Œæ•°æ®åŠ è½½é€»è¾‘
