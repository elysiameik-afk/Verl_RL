"""
HVRä¸“ç”¨Actorå®ç°

åŸºäºDataParallel Actorï¼Œä½†ä¸“é—¨ä¸ºHVRå†…ç”Ÿå¥–åŠ±æœºåˆ¶è®¾è®¡ï¼š
1. ä¿å­˜å®Œæ•´çš„logitsç”¨äºERVFè®¡ç®—
2. ç›´æ¥ä½¿ç”¨HVRå¥–åŠ±è¿›è¡Œç­–ç•¥è®­ç»ƒ
3. ç§»é™¤criticç›¸å…³é€»è¾‘ï¼Œç®€åŒ–è®­ç»ƒæµç¨‹
"""

import torch
import torch.nn.functional as F
from typing import Dict, Any, Tuple
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP

from verl.workers.actor.dp_actor import DataParallelPPOActor
from verl.trainer.hvr.hvr_core_algos import (
    calculate_hvr_rewards, 
    hvr_policy_loss,
    aggregate_hvr_metrics,
    HVRMetrics
)
from verl.utils.model import compute_position_id_with_mask
from verl.trainer.ppo.core_algos import is_main_process
from verl.utils.torch_functional import logprobs_from_logits


class HVRActor(DataParallelPPOActor):
    """
    HVRä¸“ç”¨Actor
    
    ç»§æ‰¿DataParallelPPOActorä½†ä¸“é—¨ä¸ºHVRè®¾è®¡ï¼š
    - ä¿å­˜logitsç”¨äºå†…ç”Ÿä»·å€¼è®¡ç®—
    - ä½¿ç”¨HVRå¥–åŠ±æ›¿ä»£GRPOä¼˜åŠ¿ä¼°è®¡
    - ç®€åŒ–çš„è®­ç»ƒæµç¨‹ï¼ˆæ— criticï¼‰
    """
    
    def __init__(self, config, device_name="cuda"):
        super().__init__(config, device_name)
        
        # HVRä¸“ç”¨é…ç½®
        self.hvr_alpha = self.config.get("hvr_alpha", 1.0)
        self.hvr_beta = self.config.get("hvr_beta", 0.1)
        self.hvr_lambda = self.config.get("hvr_lambda", 0.5)
        self.hvr_cliprange = self.config.get("hvr_cliprange", 0.2)
        
        if is_main_process():
            print(f"ğŸ¯ [HVR Actor] åˆå§‹åŒ–å®Œæˆ")
            print(f"ğŸ¯ [HVRå‚æ•°] Î±={self.hvr_alpha}, Î²={self.hvr_beta}, Î»={self.hvr_lambda}")
    
    def _forward_micro_batch_with_logits_hvr(
        self, 
        micro_batch, 
        temperature, 
        calculate_entropy=False
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        HVRä¸“ç”¨çš„å‰å‘ä¼ æ’­ï¼Œè¿”å›logitsç”¨äºå†…ç”Ÿä»·å€¼è®¡ç®—
        
        Returns:
            entropy: [batch_size, response_len] 
            log_probs: [batch_size, response_len]
            logits: [batch_size, response_len, vocab_size] - ç”¨äºHVR
        """
        response_length = micro_batch["responses"].size(-1)
        
        with torch.autocast(device_type=self.device_name, dtype=torch.bfloat16):
            input_ids = micro_batch["input_ids"]
            batch_size, seqlen = input_ids.shape
            attention_mask = micro_batch["attention_mask"]
            position_ids = micro_batch["position_ids"]
            
            if position_ids.dim() == 3:  # qwen2vl mrope
                position_ids = position_ids.transpose(0, 1)
            
            # HVRè¦æ±‚éremove_paddingæ¨¡å¼ä»¥è·å–å®Œæ•´logits
            if self.use_remove_padding:
                raise ValueError("HVR Actorä¸æ”¯æŒremove_paddingæ¨¡å¼ï¼Œè¯·è®¾ç½®use_remove_padding=False")
            
            # å‰å‘ä¼ æ’­è·å–logits
            output = self.actor_module(
                input_ids=input_ids,
                attention_mask=attention_mask,
                position_ids=position_ids,
                use_cache=False,
            )
            
            # å¤„ç†logitså’Œæ¦‚ç‡
            logits = output.logits
            logits.div_(temperature)
            logits = logits[:, -response_length - 1 : -1, :]  # [batch_size, response_length, vocab_size]
            
            # è®¡ç®—logæ¦‚ç‡
            log_probs = logprobs_from_logits(logits, micro_batch["responses"])
            
            # è®¡ç®—ç†µï¼ˆå¦‚æœéœ€è¦ï¼‰
            entropy = None
            if calculate_entropy:
                entropy = -(torch.softmax(logits, dim=-1) * torch.log_softmax(logits, dim=-1)).sum(dim=-1)
            
            return entropy, log_probs, logits
    
    def extract_sparse_rewards(self, data) -> torch.Tensor:
        """
        ä»æ•°æ®ä¸­æå–ç¨€ç–å¥–åŠ±R_final
        
        Args:
            data: è®­ç»ƒæ•°æ®æ‰¹æ¬¡
            
        Returns:
            sparse_rewards: [batch_size] - æ¯ä¸ªåºåˆ—çš„ç¨€ç–å¥–åŠ±
        """
        # å°è¯•ä»ä¸åŒå­—æ®µè·å–åŸå§‹å¥–åŠ±
        reward_fields = ["token_level_rewards", "token_level_scores", "rewards"]
        
        for field in reward_fields:
            if hasattr(data, 'batch') and field in data.batch:
                reward_tensor = data.batch[field]  # [batch_size, seq_len]
                
                # æå–æ¯ä¸ªåºåˆ—æœ€åä¸€ä¸ªéé›¶ä½ç½®çš„å¥–åŠ±
                sparse_rewards = []
                for i in range(reward_tensor.shape[0]):
                    nonzero_indices = torch.nonzero(reward_tensor[i]).flatten()
                    if len(nonzero_indices) > 0:
                        # å–æœ€åä¸€ä¸ªéé›¶ä½ç½®çš„å¥–åŠ±ä½œä¸ºR_final
                        last_reward_pos = nonzero_indices[-1]
                        r_final = reward_tensor[i, last_reward_pos].item()
                        sparse_rewards.append(r_final)
                    else:
                        # å¦‚æœæ²¡æœ‰éé›¶å¥–åŠ±ï¼Œä½¿ç”¨0
                        sparse_rewards.append(0.0)
                
                return torch.tensor(sparse_rewards, device=reward_tensor.device)
        
        # å¦‚æœæ‰¾ä¸åˆ°å¥–åŠ±å­—æ®µï¼Œè¿”å›é›¶å¥–åŠ±
        batch_size = data["responses"].shape[0]
        if is_main_process():
            print("âš ï¸ [HVR] æœªæ‰¾åˆ°ç¨€ç–å¥–åŠ±å­—æ®µï¼Œä½¿ç”¨é›¶å¥–åŠ±")
        return torch.zeros(batch_size, device=data["responses"].device)
    
    def update_policy(self, data) -> Dict[str, Any]:
        """
        HVRä¸“ç”¨çš„ç­–ç•¥æ›´æ–°
        
        ä½¿ç”¨å†…ç”Ÿå¥–åŠ±æœºåˆ¶ï¼Œæ— éœ€criticå’ŒGAEä¼˜åŠ¿ä¼°è®¡
        """
        metrics = {}
        
        # åŸºç¡€æ•°æ®
        responses = data["responses"]
        response_mask = data["attention_mask"][:, -responses.size(-1):]
        batch_size = responses.shape[0]
        
        # æå–ç¨€ç–å¥–åŠ±
        sparse_rewards = self.extract_sparse_rewards(data)
        
        if is_main_process():
            print(f"ğŸ¯ [HVR] å¤„ç†æ‰¹æ¬¡: batch_size={batch_size}")
            print(f"ğŸ¯ [HVR] ç¨€ç–å¥–åŠ±åˆ†å¸ƒ: {sparse_rewards.tolist()}")
        
        # å‰å‘ä¼ æ’­è·å–logits
        entropy, log_prob, response_logits = self._forward_micro_batch_with_logits_hvr(
            micro_batch=data, 
            temperature=1.0,  # HVRä½¿ç”¨å›ºå®šæ¸©åº¦
            calculate_entropy=True
        )
        
        # è®¡ç®—HVRå¥–åŠ±
        hvr_rewards_batch = []
        hvr_metrics_list = []
        
        for i in range(batch_size):
            # è·å–æœ‰æ•ˆä½ç½®
            valid_positions = torch.where(response_mask[i] > 0)[0]
            if len(valid_positions) == 0:
                continue
            
            # è·å–å½“å‰åºåˆ—çš„æ•°æ®
            valid_logits = response_logits[i, valid_positions]  # [valid_len, vocab_size]
            valid_ids = responses[i, valid_positions]  # [valid_len]
            r_final = sparse_rewards[i].item()
            
            try:
                # è®¡ç®—HVRå¥–åŠ±
                hvr_rewards, hvr_metrics = calculate_hvr_rewards(
                    response_logits=valid_logits,
                    response_ids=valid_ids,
                    R_final=r_final,
                    alpha=self.hvr_alpha,
                    beta=self.hvr_beta,
                    lambda_hvr=self.hvr_lambda,
                )
                
                # å­˜å‚¨ç»“æœ
                hvr_rewards_full = torch.zeros_like(response_mask[i], dtype=torch.float32)
                hvr_rewards_full[valid_positions] = hvr_rewards
                hvr_rewards_batch.append(hvr_rewards_full)
                hvr_metrics_list.append(hvr_metrics)
                
            except Exception as e:
                if is_main_process():
                    print(f"âš ï¸ [HVR] åºåˆ—{i}è®¡ç®—å¤±è´¥: {e}")
                # ä½¿ç”¨é›¶å¥–åŠ±ä½œä¸ºfallback
                hvr_rewards_batch.append(torch.zeros_like(response_mask[i], dtype=torch.float32))
        
        if not hvr_rewards_batch:
            if is_main_process():
                print("âš ï¸ [HVR] æ²¡æœ‰æˆåŠŸè®¡ç®—çš„HVRå¥–åŠ±")
            return {"loss": torch.tensor(0.0)}
        
        # è½¬æ¢ä¸ºæ‰¹æ¬¡å¼ é‡
        hvr_rewards_tensor = torch.stack(hvr_rewards_batch)
        
        # è®¡ç®—ç­–ç•¥æŸå¤±
        policy_loss, policy_metrics = hvr_policy_loss(
            log_probs=log_prob,
            hvr_rewards=hvr_rewards_tensor,
            response_mask=response_mask,
            cliprange=self.hvr_cliprange,
            loss_agg_mode="token-mean",
        )
        
        # èšåˆHVRæŒ‡æ ‡
        hvr_aggregated_metrics = aggregate_hvr_metrics(hvr_metrics_list)
        
        # åˆå¹¶æ‰€æœ‰æŒ‡æ ‡
        metrics.update(policy_metrics)
        metrics.update(hvr_aggregated_metrics)
        
        # æ·»åŠ åŸºç¡€æŒ‡æ ‡
        metrics.update({
            "hvr/batch_size": batch_size,
            "hvr/sparse_reward_mean": sparse_rewards.mean().item(),
            "hvr/sparse_reward_std": sparse_rewards.std().item(),
            "hvr/alpha": self.hvr_alpha,
            "hvr/beta": self.hvr_beta,
            "hvr/lambda": self.hvr_lambda,
        })
        
        if is_main_process():
            print(f"ğŸ¯ [HVR] ç­–ç•¥æŸå¤±: {policy_loss.item():.6f}")
            print(f"ğŸ¯ [HVR] HVRå¥–åŠ±å‡å€¼: {hvr_rewards_tensor.mean().item():.6f}")
            print(f"ğŸ¯ [HVR] æˆåŠŸç‡: {hvr_aggregated_metrics.get('hvr/success_rate', 0):.2f}")
        
        # åå‘ä¼ æ’­
        self.actor_module.zero_grad()
        policy_loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        if hasattr(self, 'grad_clip') and self.grad_clip > 0:
            if isinstance(self.actor_module, FSDP):
                self.actor_module.clip_grad_norm_(self.grad_clip)
            else:
                torch.nn.utils.clip_grad_norm_(self.actor_module.parameters(), self.grad_clip)
        
        # ä¼˜åŒ–å™¨æ­¥éª¤
        self.actor_optim.step()
        
        metrics["loss"] = policy_loss.item()
        return metrics

    def generate_sequences(self, prompts, **generation_kwargs):
        """
        ç”Ÿæˆåºåˆ—ï¼ˆå¤ç”¨çˆ¶ç±»å®ç°ï¼‰
        """
        return super().generate_sequences(prompts, **generation_kwargs)
