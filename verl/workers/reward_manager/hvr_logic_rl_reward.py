"""
HVR Logic RL Reward Manager

åŸºäºHVR (Hindsight Value Reshaping) çš„å¥–åŠ±ç®¡ç†å™¨ï¼Œé›†æˆERVFä»·å€¼å‡½æ•°å’Œåè§ä¹‹æ˜ä»·å€¼é‡å¡‘ã€‚
ç»§æ‰¿è‡ªLogicRLRewardManagerï¼Œåœ¨è·å–ç¨€ç–å¥–åŠ±ååº”ç”¨HVRç®—æ³•ï¼Œè¾“å‡ºé‡å¡‘åçš„å¥–åŠ±ã€‚

æ ¸å¿ƒåˆ›æ–°ï¼š
1. ERVF (ç†µæ­£åˆ™åŒ–ä»·å€¼å‡½æ•°) - åŸºäºlogitsçš„å†…ç”Ÿä»·å€¼ä¼°è®¡
2. HVR (åè§ä¹‹æ˜ä»·å€¼é‡å¡‘) - ç¨€ç–å¥–åŠ±æŒ‡å¯¼çš„ä»·å€¼è½¨è¿¹é‡å¡‘
3. GRPOç»„é—´æŠ•ç¥¨ - ä¿æŒç»„å†…ç›¸å¯¹ä¼˜åŠ¿è®¡ç®—çš„ç¨³å®šæ€§
"""

import torch
import numpy as np
from typing import Dict, Any, List
from collections import defaultdict

from verl import DataProto
from verl.workers.reward_manager.logic_rl_reward import LogicRLRewardManager, _select_rm_score_fn
from verl.trainer.ppo.core_algos import (
    calculate_ervf_value,
    calculate_hvr_rewards_for_group,
    aggregate_hvr_metrics_dict,
    is_main_process
)
from verl.workers.reward_manager.registry import register


@register("hvr_logic_rl")
class HVRLogicRLRewardManager(LogicRLRewardManager):
    """
    HVR Logic RLå¥–åŠ±ç®¡ç†å™¨

    åœ¨LogicRLåŸºç¡€ä¸Šé›†æˆHVRå†…ç”Ÿå¥–åŠ±æœºåˆ¶ï¼š
    1. è·å–ç¨€ç–å¥–åŠ±R_final (å¤ç”¨LogicRLçš„compute_score)
    2. åº”ç”¨HVRå¥–åŠ±é‡å¡‘ (ERVF + ä»·å€¼é‡å¡‘)
    3. è®¡ç®—GRPOç»„é—´ä¼˜åŠ¿ (ä¿æŒç¨³å®šæ€§)
    4. è¾“å‡ºåŒ…å«HVRä¿¡æ¯çš„å¥–åŠ±å¼ é‡
    """

    def __init__(self, tokenizer, num_examine, reward_fn_key="data_source", **kwargs):
        super().__init__(tokenizer, num_examine, reward_fn_key, **kwargs)

        # HVRå‚æ•°é…ç½® (ä»kwargsä¸­è·å–)
        self.hvr_alpha = kwargs.get("hvr_alpha", 1.0)      # æ¸©åº¦ç³»æ•°
        self.hvr_beta = kwargs.get("hvr_beta", 0.1)        # ç†µæƒ©ç½šæƒé‡
        self.hvr_lambda = kwargs.get("hvr_lambda", 0.5)    # HVRæ··åˆå› å­

        # æŒ‡æ ‡è®°å½•
        self.hvr_metrics_history = []

        if is_main_process():
            print("ğŸ¯ [HVR Manager] åˆå§‹åŒ–HVR Logic RLå¥–åŠ±ç®¡ç†å™¨")
            print(f"ğŸ¯ [HVRå‚æ•°] Î±={self.hvr_alpha}, Î²={self.hvr_beta}, Î»={self.hvr_lambda}")
            print("ğŸ¯ [HVRç‰¹æ€§] ERVFä»·å€¼å‡½æ•° + åè§ä¹‹æ˜ä»·å€¼é‡å¡‘ + GRPOç»„é—´æŠ•ç¥¨")
    
    def __call__(self, data: DataProto, return_dict: bool = False):
        """
        HVRå¥–åŠ±ç®¡ç†å™¨çš„ä¸»è¦æ¥å£

        å·¥ä½œæµç¨‹ï¼š
        1. è°ƒç”¨çˆ¶ç±»è·å–ç¨€ç–å¥–åŠ±
        2. æ£€æŸ¥æ˜¯å¦æœ‰logitsç”¨äºHVRè®¡ç®—
        3. å¦‚æœæœ‰logitsï¼Œåº”ç”¨HVRé‡å¡‘ï¼›å¦åˆ™å›é€€åˆ°åŸå§‹LogicRL
        4. è¿”å›å¥–åŠ±å¼ é‡å’Œé¢å¤–ä¿¡æ¯
        """
        if is_main_process():
            print("ğŸ¯ [HVR Manager] å¼€å§‹HVRå¥–åŠ±è®¡ç®—")

        try:
            # 1. é¦–å…ˆè°ƒç”¨çˆ¶ç±»è·å–åŸºç¡€å¥–åŠ±
            if "rm_scores" in data.batch.keys():
                # å·²ç»æœ‰é¢„è®¡ç®—çš„å¥–åŠ±ï¼Œç›´æ¥ä½¿ç”¨
                base_reward_tensor = data.batch["rm_scores"]
                reward_extra_info = {}

                if is_main_process():
                    print("ğŸ” [HVR Manager] ä½¿ç”¨é¢„è®¡ç®—çš„rm_scores")
            else:
                # éœ€è¦è®¡ç®—å¥–åŠ±
                base_result = super().__call__(data, return_dict=True)
                base_reward_tensor = base_result["reward_tensor"]
                reward_extra_info = base_result.get("reward_extra_info", {})

                if is_main_process():
                    print("ğŸ” [HVR Manager] è®¡ç®—äº†æ–°çš„å¥–åŠ±")

            # 2. æ£€æŸ¥æ˜¯å¦æœ‰rollout_log_probsç”¨äºHVRè®¡ç®—
            if "rollout_log_probs" in data.batch:
                rollout_log_probs = data.batch["rollout_log_probs"]

                if is_main_process():
                    print(f"ğŸ” [HVR Manager] æ‰¾åˆ°rollout_log_probsï¼Œå½¢çŠ¶: {rollout_log_probs.shape}")
                    print("ğŸ¯ [HVR Manager] å¼€å§‹HVRé‡å¡‘")

                # 3. åº”ç”¨HVRé‡å¡‘ (ä½¿ç”¨log_probsè€Œä¸æ˜¯logits)
                hvr_reward_tensor, hvr_extra_info = self._apply_hvr_to_rewards_with_logprobs(
                    data=data,
                    base_reward_tensor=base_reward_tensor,
                    rollout_log_probs=rollout_log_probs
                )

                # 4. åˆå¹¶é¢å¤–ä¿¡æ¯
                reward_extra_info.update(hvr_extra_info)

                if return_dict:
                    return {
                        "reward_tensor": hvr_reward_tensor,
                        "reward_extra_info": reward_extra_info
                    }
                else:
                    return hvr_reward_tensor

            else:
                if is_main_process():
                    print("âš ï¸ [HVR Manager] æœªæ‰¾åˆ°rollout_log_probsï¼Œå›é€€åˆ°åŸå§‹LogicRL")

                # å›é€€åˆ°åŸå§‹LogicRL (ç¡®ä¿åˆ—è¡¨æ ¼å¼)
                batch_size = base_reward_tensor.shape[0]
                reward_extra_info["hvr_applied"] = [False] * batch_size
                reward_extra_info["hvr_fallback_reason"] = ["no_rollout_log_probs"] * batch_size

                if return_dict:
                    return {
                        "reward_tensor": base_reward_tensor,
                        "reward_extra_info": reward_extra_info
                    }
                else:
                    return base_reward_tensor

        except Exception as e:
            if is_main_process():
                print(f"âŒ [HVR Manager] HVRè®¡ç®—å¤±è´¥: {e}")
                print("   å›é€€åˆ°åŸå§‹LogicRL")

            # å®Œå…¨å›é€€åˆ°çˆ¶ç±»
            return super().__call__(data, return_dict)

    def _apply_hvr_to_rewards(self, data, base_reward_tensor, logits):
        """
        åº”ç”¨HVRé‡å¡‘åˆ°å¥–åŠ±å¼ é‡

        Args:
            data: DataProtoå¯¹è±¡
            base_reward_tensor: [batch_size, seq_len] åŸºç¡€å¥–åŠ±å¼ é‡
            logits: [batch_size, seq_len, vocab_size] logitså¼ é‡

        Returns:
            tuple: (hvr_reward_tensor, hvr_extra_info)
        """
        # 1. æå–ç¨€ç–å¥–åŠ±
        sparse_rewards = self._extract_sparse_rewards_from_tensor(base_reward_tensor)

        if is_main_process():
            print(f"ğŸ” [HVR Manager] ç¨€ç–å¥–åŠ±åˆ†å¸ƒ: {dict(zip(*np.unique(sparse_rewards, return_counts=True)))}")

        # 2. å‡†å¤‡ç»„æ•°æ®
        group_data = self._prepare_group_data_from_batch(data, logits, sparse_rewards)

        # 3. è®¡ç®—HVRç»„å›æŠ¥
        group_returns, hvr_metrics = calculate_hvr_rewards_for_group(
            group_data=group_data,
            alpha=self.hvr_alpha,
            beta=self.hvr_beta,
            lambda_hvr=self.hvr_lambda
        )

        # 4. è®¡ç®—GRPOç»„é—´ä¼˜åŠ¿
        mean_return = sum(group_returns) / len(group_returns)
        grpo_advantages = [ret - mean_return for ret in group_returns]

        # 5. åˆ›å»ºHVRå¥–åŠ±å¼ é‡ (å°†åºåˆ—çº§ä¼˜åŠ¿åˆ†é…åˆ°tokençº§)
        hvr_reward_tensor = torch.zeros_like(base_reward_tensor, dtype=torch.float32)

        responses = data.batch["responses"]
        attention_mask = data.batch["attention_mask"]

        # è·å–responseéƒ¨åˆ†çš„mask
        response_length = responses.shape[1]
        response_mask = attention_mask[:, -response_length:]

        for i, seq_advantage in enumerate(grpo_advantages):
            # å°†åºåˆ—çº§ä¼˜åŠ¿åˆ†é…ç»™æ‰€æœ‰æœ‰æ•ˆtoken
            valid_positions = torch.where(response_mask[i] > 0)[0]
            if len(valid_positions) > 0:
                hvr_reward_tensor[i, valid_positions] = seq_advantage

        # 6. èšåˆHVRæŒ‡æ ‡
        aggregated_metrics = aggregate_hvr_metrics_dict(hvr_metrics)

        # 7. æ„å»ºé¢å¤–ä¿¡æ¯ (ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯åˆ—è¡¨æ ¼å¼)
        batch_size = len(group_returns)
        hvr_extra_info = {
            "hvr_applied": [True] * batch_size,
            "hvr_group_return_mean": [mean_return] * batch_size,
            "hvr_group_return_std": [np.std(group_returns)] * batch_size,
            "hvr_grpo_advantage_mean": [np.mean(grpo_advantages)] * batch_size,
            "hvr_grpo_advantage_std": [np.std(grpo_advantages)] * batch_size,
            "hvr_sparse_rewards": sparse_rewards,  # å·²ç»æ˜¯åˆ—è¡¨
            "hvr_alpha": [self.hvr_alpha] * batch_size,
            "hvr_beta": [self.hvr_beta] * batch_size,
            "hvr_lambda": [self.hvr_lambda] * batch_size,
        }

        # 8. æ·»åŠ HVRæŒ‡æ ‡
        hvr_extra_info.update(aggregated_metrics)

        # 9. è®°å½•æŒ‡æ ‡å†å²
        self.hvr_metrics_history.append(aggregated_metrics)

        if is_main_process():
            print(f"âœ… [HVR Manager] HVRé‡å¡‘å®Œæˆ")
            print(f"   ç»„å¹³å‡å›æŠ¥: {mean_return:.4f}")
            print(f"   GRPOä¼˜åŠ¿èŒƒå›´: [{min(grpo_advantages):.4f}, {max(grpo_advantages):.4f}]")
            print(f"   HVRæˆåŠŸç‡: {aggregated_metrics.get('hvr/success_rate', 0):.2f}")

        return hvr_reward_tensor, hvr_extra_info

    def _apply_hvr_to_rewards_with_logprobs(self, data, base_reward_tensor, rollout_log_probs):
        """
        ä½¿ç”¨rollout_log_probsåº”ç”¨HVRé‡å¡‘ (é€‚é…vLLM rolloutè¾“å‡º)

        Args:
            data: DataProtoå¯¹è±¡
            base_reward_tensor: [batch_size, seq_len] åŸºç¡€å¥–åŠ±å¼ é‡
            rollout_log_probs: [batch_size, seq_len] rolloutçš„logæ¦‚ç‡

        Returns:
            tuple: (hvr_reward_tensor, hvr_extra_info)
        """
        # 1. æå–ç¨€ç–å¥–åŠ±
        sparse_rewards = self._extract_sparse_rewards_from_tensor(base_reward_tensor)

        if is_main_process():
            print(f"ğŸ” [HVR Manager] ç¨€ç–å¥–åŠ±åˆ†å¸ƒ: {dict(zip(*np.unique(sparse_rewards, return_counts=True)))}")

        # 2. å‡†å¤‡ç»„æ•°æ® (ä½¿ç”¨log_probsè€Œä¸æ˜¯logits)
        group_data = self._prepare_group_data_with_logprobs(data, rollout_log_probs, sparse_rewards)

        # 3. è®¡ç®—HVRç»„å›æŠ¥ (ä½¿ç”¨ç®€åŒ–çš„HVRç®—æ³•)
        group_returns, hvr_metrics = self._calculate_hvr_returns_from_logprobs(
            group_data=group_data,
            alpha=self.hvr_alpha,
            beta=self.hvr_beta,
            lambda_hvr=self.hvr_lambda
        )

        # 4. è®¡ç®—GRPOç»„é—´ä¼˜åŠ¿
        mean_return = sum(group_returns) / len(group_returns)
        grpo_advantages = [ret - mean_return for ret in group_returns]

        # 5. åˆ›å»ºHVRå¥–åŠ±å¼ é‡
        hvr_reward_tensor = torch.zeros_like(base_reward_tensor, dtype=torch.float32)

        responses = data.batch["responses"]
        attention_mask = data.batch["attention_mask"]

        # è·å–responseéƒ¨åˆ†çš„mask
        response_length = responses.shape[1]
        response_mask = attention_mask[:, -response_length:]

        for i, seq_advantage in enumerate(grpo_advantages):
            # å°†åºåˆ—çº§ä¼˜åŠ¿åˆ†é…ç»™æ‰€æœ‰æœ‰æ•ˆtoken
            valid_positions = torch.where(response_mask[i] > 0)[0]
            if len(valid_positions) > 0:
                hvr_reward_tensor[i, valid_positions] = seq_advantage

        # 6. èšåˆHVRæŒ‡æ ‡ (ä½¿ç”¨å®‰å…¨çš„èšåˆæ–¹å¼)
        try:
            aggregated_metrics = aggregate_hvr_metrics_dict(hvr_metrics)
        except Exception as e:
            if is_main_process():
                print(f"âš ï¸ [HVR Manager] æŒ‡æ ‡èšåˆå¤±è´¥: {e}ï¼Œä½¿ç”¨ç®€åŒ–æŒ‡æ ‡")
            # ä½¿ç”¨ç®€åŒ–çš„æŒ‡æ ‡æ ¼å¼
            aggregated_metrics = {
                'hvr/success_rate': hvr_metrics['successful_count'] / hvr_metrics['total_count'],
                'hvr/total_sequences': hvr_metrics['total_count'],
                'hvr/successful_sequences': hvr_metrics['successful_count'],
            }

        # 7. æ„å»ºé¢å¤–ä¿¡æ¯ (ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯åˆ—è¡¨æ ¼å¼)
        batch_size = len(group_returns)
        hvr_extra_info = {
            "hvr_applied": [True] * batch_size,
            "hvr_method": ["logprobs_based"] * batch_size,
            "hvr_group_return_mean": [mean_return] * batch_size,
            "hvr_group_return_std": [np.std(group_returns)] * batch_size,
            "hvr_grpo_advantage_mean": [np.mean(grpo_advantages)] * batch_size,
            "hvr_grpo_advantage_std": [np.std(grpo_advantages)] * batch_size,
            "hvr_sparse_rewards": sparse_rewards,  # å·²ç»æ˜¯åˆ—è¡¨
            "hvr_alpha": [self.hvr_alpha] * batch_size,
            "hvr_beta": [self.hvr_beta] * batch_size,
            "hvr_lambda": [self.hvr_lambda] * batch_size,
        }

        # 8. æ·»åŠ HVRæŒ‡æ ‡
        hvr_extra_info.update(aggregated_metrics)

        # 9. è®°å½•æŒ‡æ ‡å†å²
        self.hvr_metrics_history.append(aggregated_metrics)

        if is_main_process():
            print(f"âœ… [HVR Manager] HVRé‡å¡‘å®Œæˆ (åŸºäºlog_probs)")
            print(f"   ç»„å¹³å‡å›æŠ¥: {mean_return:.4f}")
            print(f"   GRPOä¼˜åŠ¿èŒƒå›´: [{min(grpo_advantages):.4f}, {max(grpo_advantages):.4f}]")
            print(f"   HVRæˆåŠŸç‡: {aggregated_metrics.get('hvr/success_rate', 0):.2f}")

        return hvr_reward_tensor, hvr_extra_info

    def _prepare_group_data_with_logprobs(self, data, rollout_log_probs, sparse_rewards):
        """ä½¿ç”¨log_probså‡†å¤‡HVRç»„æ•°æ®"""
        responses = data.batch["responses"]
        attention_mask = data.batch["attention_mask"]

        # è·å–responseéƒ¨åˆ†çš„æ•°æ®
        response_length = responses.shape[1]
        response_mask = attention_mask[:, -response_length:]
        response_log_probs = rollout_log_probs  # [batch_size, response_len]

        group_data = []

        for i in range(len(sparse_rewards)):
            # è·å–æœ‰æ•ˆä½ç½®
            valid_positions = torch.where(response_mask[i] > 0)[0]
            if len(valid_positions) == 0:
                if is_main_process():
                    print(f"âš ï¸ [HVR Manager] åºåˆ—{i}æ²¡æœ‰æœ‰æ•ˆtokenï¼Œè·³è¿‡")
                continue

            # æå–æœ‰æ•ˆçš„log_probså’Œtoken IDs
            valid_log_probs = response_log_probs[i, valid_positions]  # [valid_len]
            valid_ids = responses[i, valid_positions]  # [valid_len]
            r_final = sparse_rewards[i]

            group_data.append({
                'log_probs': valid_log_probs,
                'ids': valid_ids,
                'r_final': r_final
            })

            if is_main_process() and i == 0:
                print(f"ğŸ” [HVR Manager] åºåˆ—{i}: æœ‰æ•ˆé•¿åº¦={len(valid_positions)}, R_final={r_final}")

        return group_data

    def _calculate_hvr_returns_from_logprobs(self, group_data, alpha, beta, lambda_hvr):
        """
        åŸºäºlog_probsè®¡ç®—HVRå›æŠ¥ (ç®€åŒ–ç‰ˆæœ¬)

        ç”±äºæ²¡æœ‰åŸå§‹logitsï¼Œæˆ‘ä»¬ä½¿ç”¨ç®€åŒ–çš„HVRç®—æ³•ï¼š
        1. ä½¿ç”¨log_probsä½œä¸ºä»·å€¼çš„ä»£ç†
        2. åº”ç”¨ä»·å€¼é‡å¡‘
        3. è®¡ç®—æ€»å›æŠ¥
        """
        group_returns = []
        hvr_metrics = {
            'ervf_values': [],      # å…¼å®¹aggregate_hvr_metrics_dict
            'entropies': [],        # å…¼å®¹aggregate_hvr_metrics_dict
            'hvr_rewards': [],      # å…¼å®¹aggregate_hvr_metrics_dict
            'r_finals': [],
            'v_targets': [],
            'sequence_lengths': [],
            'successful_count': 0,
            'total_count': len(group_data)
        }

        if is_main_process():
            print(f"ğŸ¯ [HVR Manager] å¤„ç†ç»„æ•°æ®: {len(group_data)} ä¸ªåºåˆ— (åŸºäºlog_probs)")

        for seq_idx, d in enumerate(group_data):
            try:
                # æå–æ•°æ®
                log_probs = d['log_probs']  # [sequence_length]
                ids = d['ids']              # [sequence_length]
                r_final = d['r_final']      # scalar

                sequence_length = log_probs.shape[0]

                if is_main_process() and seq_idx == 0:
                    print(f"ğŸ” [HVR Manager] åºåˆ—{seq_idx}: é•¿åº¦={sequence_length}, R_final={r_final}")

                # ç®€åŒ–çš„HVRè®¡ç®—ï¼šä½¿ç”¨log_probsä½œä¸ºä»·å€¼ä»£ç†
                # V_proxy(t) = alpha * log_prob(t) (ç®€åŒ–çš„å†…ç”Ÿä»·å€¼)
                v_proxy_list = [alpha * lp.item() for lp in log_probs]
                v_proxy_list.append(0.0)  # ç»ˆæ­¢çŠ¶æ€

                # ä»·å€¼é‡å¡‘
                V_max = max(v_proxy_list[:-1])
                V_min = min(v_proxy_list[:-1])

                p = (r_final + 3.0) / 6.0
                p = max(0.0, min(1.0, p))
                V_target = (1.0 - p) * V_min + p * V_max

                # é‡å¡‘åä»·å€¼
                V_hvr_list = [(1.0 - lambda_hvr) * v + lambda_hvr * V_target for v in v_proxy_list]

                # è®¡ç®—ç¨ å¯†å¥–åŠ±
                r_hvr_list = []
                for t in range(sequence_length):
                    # ç®€åŒ–çš„HVRå¥–åŠ±ï¼šr_hvr_t = alpha * log_prob_t + V_hvr[t] - V_hvr[t+1]
                    r_hvr_t = alpha * log_probs[t].item() + V_hvr_list[t] - V_hvr_list[t + 1]
                    r_hvr_list.append(r_hvr_t)

                # æ·»åŠ æœ€ç»ˆå¥–åŠ±
                r_hvr_list[-1] += r_final

                # è®¡ç®—æ€»å›æŠ¥
                total_return = sum(r_hvr_list)
                group_returns.append(total_return)

                # æ”¶é›†æŒ‡æ ‡ (å…¼å®¹aggregate_hvr_metrics_dictæ ¼å¼)
                hvr_metrics['ervf_values'].extend(v_proxy_list[:-1])  # ä½¿ç”¨v_proxyä½œä¸ºERVFä»£ç†
                hvr_metrics['entropies'].extend([0.0] * sequence_length)  # ç®€åŒ–ç‰ˆæ²¡æœ‰ç†µè®¡ç®—
                hvr_metrics['hvr_rewards'].extend(r_hvr_list)
                hvr_metrics['r_finals'].append(r_final)
                hvr_metrics['v_targets'].append(V_target)
                hvr_metrics['sequence_lengths'].append(sequence_length)
                hvr_metrics['successful_count'] += 1

                if is_main_process() and seq_idx == 0:
                    print(f"âœ… [HVR Manager] åºåˆ—{seq_idx}: æ€»å›æŠ¥={total_return:.4f}, V_target={V_target:.4f}")
                    print(f"   HVRå¥–åŠ±èŒƒå›´: [{min(r_hvr_list):.4f}, {max(r_hvr_list):.4f}]")

            except Exception as e:
                if is_main_process():
                    print(f"âŒ [HVR Manager] åºåˆ—{seq_idx}å¤„ç†å¤±è´¥: {e}")
                group_returns.append(0.0)

        if is_main_process():
            success_rate = hvr_metrics['successful_count'] / hvr_metrics['total_count']
            print(f"ğŸ¯ [HVR Manager] ç»„å¤„ç†å®Œæˆ: æˆåŠŸç‡={success_rate:.2f}, å¹³å‡å›æŠ¥={np.mean(group_returns):.4f}")

        return group_returns, hvr_metrics

    def _extract_sparse_rewards_from_tensor(self, reward_tensor):
        """ä»å¥–åŠ±å¼ é‡ä¸­æå–ç¨€ç–å¥–åŠ±"""
        sparse_rewards = []

        for i in range(reward_tensor.shape[0]):
            nonzero_indices = torch.nonzero(reward_tensor[i]).flatten()
            if len(nonzero_indices) > 0:
                # å–æœ€åä¸€ä¸ªéé›¶ä½ç½®çš„å¥–åŠ±ä½œä¸ºR_final
                last_reward_pos = nonzero_indices[-1]
                r_final = reward_tensor[i, last_reward_pos].item()
                sparse_rewards.append(r_final)
            else:
                # å¦‚æœæ²¡æœ‰éé›¶å¥–åŠ±ï¼Œä½¿ç”¨0
                sparse_rewards.append(0.0)

        return sparse_rewards

    def _prepare_group_data_from_batch(self, data, logits, sparse_rewards):
        """ä»batchæ•°æ®å‡†å¤‡HVRç»„æ•°æ®"""
        responses = data.batch["responses"]
        attention_mask = data.batch["attention_mask"]

        # è·å–responseéƒ¨åˆ†çš„æ•°æ®
        response_length = responses.shape[1]
        response_mask = attention_mask[:, -response_length:]
        response_logits = logits[:, -response_length:, :]  # [batch_size, response_len, vocab_size]

        group_data = []

        for i in range(len(sparse_rewards)):
            # è·å–æœ‰æ•ˆä½ç½®
            valid_positions = torch.where(response_mask[i] > 0)[0]
            if len(valid_positions) == 0:
                if is_main_process():
                    print(f"âš ï¸ [HVR Manager] åºåˆ—{i}æ²¡æœ‰æœ‰æ•ˆtokenï¼Œè·³è¿‡")
                continue

            # æå–æœ‰æ•ˆçš„logitså’Œtoken IDs
            valid_logits = response_logits[i, valid_positions]  # [valid_len, vocab_size]
            valid_ids = responses[i, valid_positions]  # [valid_len]
            r_final = sparse_rewards[i]

            group_data.append({
                'logits': valid_logits,
                'ids': valid_ids,
                'r_final': r_final
            })

            if is_main_process() and i == 0:
                print(f"ğŸ” [HVR Manager] åºåˆ—{i}: æœ‰æ•ˆé•¿åº¦={len(valid_positions)}, R_final={r_final}")

        return group_data

    def get_hvr_metrics_summary(self):
        """è·å–HVRæŒ‡æ ‡æ‘˜è¦ (ç”¨äºæœ€ç»ˆåˆ†æ)"""
        if not self.hvr_metrics_history:
            return {}

        # èšåˆæ‰€æœ‰å†å²æŒ‡æ ‡
        summary = {}

        # æ”¶é›†æ‰€æœ‰æ•°å€¼æŒ‡æ ‡
        all_metrics = defaultdict(list)
        for metrics in self.hvr_metrics_history:
            for key, value in metrics.items():
                if isinstance(value, (int, float)):
                    all_metrics[key].append(value)

        # è®¡ç®—ç»Ÿè®¡æ‘˜è¦
        for key, values in all_metrics.items():
            summary[f"{key}_mean"] = np.mean(values)
            summary[f"{key}_std"] = np.std(values)
            summary[f"{key}_min"] = np.min(values)
            summary[f"{key}_max"] = np.max(values)

        summary["hvr_total_batches"] = len(self.hvr_metrics_history)

        return summary
    
    def _extract_logits_from_rollout(self, rollout_output):
        """ä»rolloutè¾“å‡ºä¸­æå–logits"""
        # æ£€æŸ¥å¯èƒ½çš„logitså­—æ®µ
        logits_fields = ["logits", "response_logits", "output_logits"]
        
        for field in logits_fields:
            if field in rollout_output:
                logits = rollout_output[field]
                if is_main_process():
                    print(f"ğŸ” [HVR Manager] æ‰¾åˆ°logitså­—æ®µ: {field}, å½¢çŠ¶: {logits.shape}")
                return logits
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°logitsï¼ŒæŠ›å‡ºé”™è¯¯
        available_fields = list(rollout_output.keys())
        raise ValueError(f"æœªæ‰¾åˆ°logitså­—æ®µã€‚å¯ç”¨å­—æ®µ: {available_fields}")
    
    def _extract_sparse_rewards(self, sparse_reward_result):
        """ä»LogicRLç»“æœä¸­æå–ç¨€ç–å¥–åŠ±åˆ—è¡¨"""
        # æ£€æŸ¥å¯èƒ½çš„å¥–åŠ±å­—æ®µ
        reward_fields = ["token_level_rewards", "token_level_scores", "rewards"]
        
        for field in reward_fields:
            if field in sparse_reward_result:
                reward_tensor = sparse_reward_result[field]  # [batch_size, seq_len]
                
                # æå–æ¯ä¸ªåºåˆ—æœ€åä¸€ä¸ªéé›¶ä½ç½®çš„å¥–åŠ±
                sparse_rewards = []
                for i in range(reward_tensor.shape[0]):
                    nonzero_indices = torch.nonzero(reward_tensor[i]).flatten()
                    if len(nonzero_indices) > 0:
                        # å–æœ€åä¸€ä¸ªéé›¶ä½ç½®çš„å¥–åŠ±ä½œä¸ºR_final
                        last_reward_pos = nonzero_indices[-1]
                        r_final = reward_tensor[i, last_reward_pos].item()
                        sparse_rewards.append(r_final)
                    else:
                        # å¦‚æœæ²¡æœ‰éé›¶å¥–åŠ±ï¼Œä½¿ç”¨0
                        sparse_rewards.append(0.0)
                
                if is_main_process():
                    print(f"ğŸ” [HVR Manager] ä»{field}æå–ç¨€ç–å¥–åŠ±: {sparse_rewards}")
                
                return sparse_rewards
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¥–åŠ±å­—æ®µï¼Œä½¿ç”¨é›¶å¥–åŠ±
        batch_size = len(rollout_output.get("responses", [1]))
        if is_main_process():
            print(f"âš ï¸ [HVR Manager] æœªæ‰¾åˆ°ç¨€ç–å¥–åŠ±å­—æ®µï¼Œä½¿ç”¨é›¶å¥–åŠ±")
            print(f"   å¯ç”¨å­—æ®µ: {list(sparse_reward_result.keys())}")
        
        return [0.0] * batch_size

    def _apply_hvr_reshaping(self, rollout_output, logits, sparse_rewards):
        """
        åº”ç”¨HVRå¥–åŠ±é‡å¡‘çš„æ ¸å¿ƒæ–¹æ³•

        Args:
            rollout_output: rolloutè¾“å‡ºæ•°æ®
            logits: [batch_size, seq_len, vocab_size] - æ¨¡å‹logits
            sparse_rewards: List[float] - ç¨€ç–å¥–åŠ±R_finalåˆ—è¡¨

        Returns:
            tuple: (hvr_advantages, hvr_metrics)
        """
        if is_main_process():
            print(f"ğŸ¯ [HVR Manager] å¼€å§‹HVRé‡å¡‘, ç»„å¤§å°: {len(sparse_rewards)}")

        # 1. å‡†å¤‡ç»„æ•°æ®
        group_data = self._prepare_group_data(rollout_output, logits, sparse_rewards)

        # 2. è®¡ç®—HVRç»„å›æŠ¥
        group_returns, hvr_metrics = calculate_hvr_rewards_for_group(
            group_data=group_data,
            alpha=self.hvr_alpha,
            beta=self.hvr_beta,
            lambda_hvr=self.hvr_lambda
        )

        # 3. è®¡ç®—GRPOç»„é—´ä¼˜åŠ¿ (ä¿æŒGRPOçš„æ ¸å¿ƒç¨³å®šæ€§)
        mean_return = sum(group_returns) / len(group_returns)
        grpo_advantages = [ret - mean_return for ret in group_returns]

        # 4. å°†åºåˆ—çº§ä¼˜åŠ¿æ‰©å±•åˆ°tokençº§
        responses = rollout_output["responses"]
        attention_mask = rollout_output["attention_mask"]

        # è·å–responseéƒ¨åˆ†çš„mask
        response_length = responses.shape[1]
        response_mask = attention_mask[:, -response_length:]

        # åˆ›å»ºtokençº§ä¼˜åŠ¿å¼ é‡
        hvr_advantages = torch.zeros_like(response_mask, dtype=torch.float32)

        for i, seq_advantage in enumerate(grpo_advantages):
            # å°†åºåˆ—çº§ä¼˜åŠ¿åˆ†é…ç»™æ‰€æœ‰æœ‰æ•ˆtoken
            valid_positions = torch.where(response_mask[i] > 0)[0]
            if len(valid_positions) > 0:
                hvr_advantages[i, valid_positions] = seq_advantage

        # 5. èšåˆHVRæŒ‡æ ‡
        aggregated_metrics = aggregate_hvr_metrics_dict(hvr_metrics)

        # 6. æ·»åŠ GRPOå’Œç®¡ç†å™¨çº§åˆ«çš„æŒ‡æ ‡
        aggregated_metrics.update({
            'hvr/group_return_mean': mean_return,
            'hvr/group_return_std': np.std(group_returns),
            'hvr/grpo_advantage_mean': np.mean(grpo_advantages),
            'hvr/grpo_advantage_std': np.std(grpo_advantages),
            'hvr/manager_alpha': self.hvr_alpha,
            'hvr/manager_beta': self.hvr_beta,
            'hvr/manager_lambda': self.hvr_lambda,
            'hvr/token_advantage_mean': hvr_advantages[response_mask > 0].mean().item(),
            'hvr/token_advantage_std': hvr_advantages[response_mask > 0].std().item(),
        })

        if is_main_process():
            print(f"âœ… [HVR Manager] HVRé‡å¡‘å®Œæˆ")
            print(f"   ç»„å¹³å‡å›æŠ¥: {mean_return:.4f}")
            print(f"   GRPOä¼˜åŠ¿èŒƒå›´: [{min(grpo_advantages):.4f}, {max(grpo_advantages):.4f}]")
            print(f"   Tokenä¼˜åŠ¿èŒƒå›´: [{hvr_advantages[response_mask > 0].min().item():.4f}, {hvr_advantages[response_mask > 0].max().item():.4f}]")

        return hvr_advantages, aggregated_metrics

    def _prepare_group_data(self, rollout_output, logits, sparse_rewards):
        """ä¸ºHVRå‡†å¤‡ç»„æ•°æ®"""
        responses = rollout_output["responses"]
        attention_mask = rollout_output["attention_mask"]

        # è·å–responseéƒ¨åˆ†çš„æ•°æ®
        response_length = responses.shape[1]
        response_mask = attention_mask[:, -response_length:]
        response_logits = logits[:, -response_length:, :]  # [batch_size, response_len, vocab_size]

        group_data = []

        for i in range(len(sparse_rewards)):
            # è·å–æœ‰æ•ˆä½ç½®
            valid_positions = torch.where(response_mask[i] > 0)[0]
            if len(valid_positions) == 0:
                if is_main_process():
                    print(f"âš ï¸ [HVR Manager] åºåˆ—{i}æ²¡æœ‰æœ‰æ•ˆtokenï¼Œè·³è¿‡")
                continue

            # æå–æœ‰æ•ˆçš„logitså’Œtoken IDs
            valid_logits = response_logits[i, valid_positions]  # [valid_len, vocab_size]
            valid_ids = responses[i, valid_positions]  # [valid_len]
            r_final = sparse_rewards[i]

            group_data.append({
                'logits': valid_logits,
                'ids': valid_ids,
                'r_final': r_final
            })

            if is_main_process() and i == 0:
                print(f"ğŸ” [HVR Manager] åºåˆ—{i}: æœ‰æ•ˆé•¿åº¦={len(valid_positions)}, R_final={r_final}")

        return group_data

    def get_hvr_metrics_summary(self):
        """è·å–HVRæŒ‡æ ‡æ‘˜è¦ (ç”¨äºæœ€ç»ˆåˆ†æ)"""
        if not self.hvr_metrics_history:
            return {}

        # èšåˆæ‰€æœ‰å†å²æŒ‡æ ‡
        summary = {}

        # æ”¶é›†æ‰€æœ‰æ•°å€¼æŒ‡æ ‡
        all_metrics = {}
        for metrics in self.hvr_metrics_history:
            for key, value in metrics.items():
                if isinstance(value, (int, float)):
                    if key not in all_metrics:
                        all_metrics[key] = []
                    all_metrics[key].append(value)

        # è®¡ç®—ç»Ÿè®¡æ‘˜è¦
        for key, values in all_metrics.items():
            summary[f"{key}_mean"] = np.mean(values)
            summary[f"{key}_std"] = np.std(values)
            summary[f"{key}_min"] = np.min(values)
            summary[f"{key}_max"] = np.max(values)

        summary["hvr_total_batches"] = len(self.hvr_metrics_history)

        return summary