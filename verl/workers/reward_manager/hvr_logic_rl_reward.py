"""
HVR Logic RL Reward Manager

åŸºäºHVR (Hybrid Value Reshaping) çš„å¥–åŠ±ç®¡ç†å™¨ï¼Œé›†æˆERVFä»·å€¼å‡½æ•°å’Œæ··åˆä»·å€¼é‡å¡‘ã€‚
ç»§æ‰¿è‡ªLogicRLRewardManagerï¼Œåœ¨è·å–ç¨€ç–å¥–åŠ±ååº”ç”¨HVRç®—æ³•ï¼Œè¾“å‡ºé‡å¡‘åçš„å¥–åŠ±ã€‚

æ ¸å¿ƒåˆ›æ–°ï¼š
1. ERVF (ç†µæ­£åˆ™åŒ–ä»·å€¼å‡½æ•°) - åŸºäºlogitsçš„å†…ç”Ÿä»·å€¼ä¼°è®¡
2. HVR (æ··åˆä»·å€¼é‡å¡‘) - ç¨€ç–å¥–åŠ±æŒ‡å¯¼çš„ä»·å€¼è½¨è¿¹é‡å¡‘
3. Z-scoreå½’ä¸€åŒ– - ä¿æŒæ•°å€¼ç¨³å®šæ€§
"""

import torch
import numpy as np
from typing import Dict, Any, List, Tuple
from collections import defaultdict

from verl import DataProto
from verl.workers.reward_manager.logic_rl_reward import LogicRLRewardManager, _select_rm_score_fn
from verl.workers.reward_manager.registry import register


def calculate_ervf_value(logits: torch.Tensor, alpha: float, beta: float) -> float:
    """
    è®¡ç®—åŸºäºERVFçš„å†…ç”Ÿä»·å€¼å‡½æ•°

    Args:
        logits: ä¸€ç»´å¼ é‡ (vocab_size,) - æ¨¡å‹åœ¨æŸçŠ¶æ€çš„åŸå§‹logits
        alpha: æ¸©åº¦ç³»æ•°ï¼Œè°ƒèŠ‚logsumexpå¹³æ»‘åº¦
        beta: ç†µæƒ©ç½šæƒé‡ï¼Œè°ƒèŠ‚ä¸ç¡®å®šæ€§æƒ©ç½šåŠ›åº¦

    Returns:
        V_ervf: ç†µæ­£åˆ™åŒ–åçš„å†…ç”Ÿä»·å€¼
    """
    # æ•°å€¼ç¨³å®šåŒ–
    logits = torch.clamp(logits, min=-10, max=10)

    # è®¡ç®—å†…ç”Ÿä»·å€¼ V_endo
    V_endo = alpha * torch.logsumexp(logits / alpha, dim=-1)

    # è®¡ç®—ç­–ç•¥ç†µ H
    probs = torch.nn.functional.softmax(logits, dim=-1)
    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)
    H = -torch.sum(probs * log_probs, dim=-1)

    # æœ€ç»ˆç†µæ­£åˆ™åŒ–ä»·å€¼
    V_ervf = V_endo - beta * H

    return V_ervf.item()


def calculate_hvr_rewards_for_group(
    group_data: List[Dict],
    alpha: float = 1.0,
    beta: float = 0.1,
    lambda_hvr: float = 0.5,
    use_zscore: bool = True,
    target_scale: float = 3.0
) -> Tuple[List[float], Dict[str, float]]:
    """
    ä¸ºä¸€ç»„responseè®¡ç®—HVRå¥–åŠ±

    Args:
        group_data: ç»„å†…æ‰€æœ‰responseçš„æ•°æ®
        alpha, beta, lambda_hvr: HVRç®—æ³•è¶…å‚æ•°
        use_zscore: æ˜¯å¦ä½¿ç”¨Z-scoreå½’ä¸€åŒ–
        target_scale: Z-scoreåçš„ç›®æ ‡æ ‡å‡†å·®

    Returns:
        group_returns: æ¯ä¸ªresponseçš„æœ€ç»ˆå¥–åŠ±
        metrics: ç›‘æ§æŒ‡æ ‡
    """
    group_returns = []
    metrics = {
        'v_ervf_values': [],
        'v_target_values': [],
        'v_hvr_values': [],
        'raw_returns': [],
        'r_final_values': [],
        'external_scores': []
    }

    for response_data in group_data:
        logits = response_data['logits']  # (seq_len, vocab_size)
        r_final = response_data['r_final']  # å¤–éƒ¨ç¨€ç–å¥–åŠ±
        response_ids = response_data['ids']  # (seq_len,)
        external_score = response_data.get('external_score', r_final)  # åŸå§‹å¤–éƒ¨åˆ†æ•°

        # 1. è®¡ç®—V_ervfè½¨è¿¹
        V_ervf_list = []
        for step_logits in logits:
            V_ervf = calculate_ervf_value(step_logits, alpha, beta)
            V_ervf_list.append(V_ervf)

        # 2. åºåˆ—çº§Z-scoreå½’ä¸€åŒ– (ç¨³å®šå•ä¸ªåºåˆ—)
        if len(V_ervf_list) > 1 and use_zscore:
            V_ervf_tensor = torch.tensor(V_ervf_list)
            V_ervf_mean = V_ervf_tensor.mean()
            V_ervf_std = V_ervf_tensor.std() + 1e-8
            V_ervf_normalized = (V_ervf_tensor - V_ervf_mean) / V_ervf_std
        else:
            V_ervf_normalized = torch.tensor(V_ervf_list)

        # 3. é‡å¡‘ç›®æ ‡ (ç›´æ¥ä½¿ç”¨å¤–éƒ¨å¥–åŠ±ï¼Œä¸å½’ä¸€åŒ–)
        V_target = r_final * 0.3  # é€‚åº¦ç¼©æ”¾å¤–éƒ¨å¥–åŠ±å½±å“

        # 4. é‡å¡‘ä»·å€¼è½¨è¿¹
        V_hvr_list = [(1.0 - lambda_hvr) * v.item() + lambda_hvr * V_target
                      for v in V_ervf_normalized]

        # 5. åˆ†è§£ä¸ºç¨ å¯†å¥–åŠ±
        r_hvr_list = []
        for t in range(len(V_hvr_list) - 1):
            # è®¡ç®—å½“å‰tokençš„logæ¦‚ç‡
            step_logits = logits[t]
            token_id = response_ids[t]
            log_probs = torch.nn.functional.log_softmax(step_logits, dim=-1)
            log_prob_t = log_probs[token_id].item()

            # HVRç¨ å¯†å¥–åŠ±å…¬å¼
            r_hvr_t = alpha * log_prob_t + V_hvr_list[t] - V_hvr_list[t+1]
            # æ§åˆ¶å•æ­¥å¥–åŠ±èŒƒå›´
            r_hvr_t = torch.clamp(torch.tensor(r_hvr_t), min=-1.0, max=1.0).item()
            r_hvr_list.append(r_hvr_t)

        # 6. è®¡ç®—æ€»å›æŠ¥
        total_hvr_reward = sum(r_hvr_list)
        raw_total_return = total_hvr_reward + r_final

        # æ”¶é›†æŒ‡æ ‡
        metrics['v_ervf_values'].extend(V_ervf_list)
        metrics['v_target_values'].append(V_target)
        metrics['v_hvr_values'].extend(V_hvr_list)
        metrics['raw_returns'].append(raw_total_return)
        metrics['r_final_values'].append(r_final)
        metrics['external_scores'].append(external_score)

        group_returns.append(raw_total_return)

    # 7. ç»„çº§Z-scoreå½’ä¸€åŒ–
    if len(group_returns) > 1 and use_zscore:
        returns_tensor = torch.tensor(group_returns)
        returns_mean = returns_tensor.mean()
        returns_std = returns_tensor.std() + 1e-8
        returns_normalized = (returns_tensor - returns_mean) / returns_std

        # ç¼©æ”¾åˆ°ç›®æ ‡èŒƒå›´
        final_returns = (returns_normalized * target_scale).tolist()

        # æœ€ç»ˆèŒƒå›´æ§åˆ¶
        final_returns = [torch.clamp(torch.tensor(r), min=-6.0, max=6.0).item()
                        for r in final_returns]
    else:
        final_returns = [torch.clamp(torch.tensor(r), min=-6.0, max=6.0).item()
                        for r in group_returns]

    # è®¡ç®—æ±‡æ€»æŒ‡æ ‡
    summary_metrics = {
        'v_ervf_mean': np.mean(metrics['v_ervf_values']) if metrics['v_ervf_values'] else 0.0,
        'v_target_mean': np.mean(metrics['v_target_values']) if metrics['v_target_values'] else 0.0,
        'v_hvr_mean': np.mean(metrics['v_hvr_values']) if metrics['v_hvr_values'] else 0.0,
        'raw_return_mean': np.mean(metrics['raw_returns']) if metrics['raw_returns'] else 0.0,
        'raw_return_std': np.std(metrics['raw_returns']) if metrics['raw_returns'] else 0.0,
        'final_return_mean': np.mean(final_returns),
        'r_final_mean': np.mean(metrics['r_final_values']) if metrics['r_final_values'] else 0.0,
        'external_score_mean': np.mean(metrics['external_scores']) if metrics['external_scores'] else 0.0,
    }

    return final_returns, summary_metrics


@register("hvr_logic_rl")
class HVRLogicRLRewardManager(LogicRLRewardManager):
    """
    HVR Logic RLå¥–åŠ±ç®¡ç†å™¨

    åœ¨LogicRLåŸºç¡€ä¸Šé›†æˆHVRæ··åˆä»·å€¼é‡å¡‘æœºåˆ¶ï¼š
    1. è·å–ç¨€ç–å¥–åŠ±R_final (å¤ç”¨LogicRLçš„compute_score)
    2. åº”ç”¨HVRå¥–åŠ±é‡å¡‘ (ERVF + ä»·å€¼é‡å¡‘)
    3. Z-scoreå½’ä¸€åŒ–ä¿æŒæ•°å€¼ç¨³å®š
    4. è¾“å‡ºåŒ…å«HVRä¿¡æ¯çš„å¥–åŠ±å¼ é‡
    """

    def __init__(self, tokenizer, num_examine, reward_fn_key="data_source", **kwargs):
        super().__init__(tokenizer, num_examine, reward_fn_key, **kwargs)

        # HVRè¶…å‚æ•° - ä»kwargsä¸­è·å–ï¼Œæä¾›é»˜è®¤å€¼
        self.alpha = kwargs.get('alpha', 1.0)
        self.beta = kwargs.get('beta', 0.1)
        self.lambda_hvr = kwargs.get('lambda_hvr', 0.5)
        self.use_zscore = kwargs.get('use_zscore', True)
        self.target_scale = kwargs.get('target_scale', 3.0)

        print(f"ğŸ¯ [HVRåˆå§‹åŒ–] alpha={self.alpha}, beta={self.beta}, lambda_hvr={self.lambda_hvr}")
        print(f"ğŸ¯ [HVRåˆå§‹åŒ–] use_zscore={self.use_zscore}, target_scale={self.target_scale}")

    def __call__(self, data: DataProto, return_dict: bool = False):
        """
        ä¸»è¦æ¥å£ï¼šè®¡ç®—HVRå¥–åŠ±
        """
        # æ£€æŸ¥æ˜¯å¦æœ‰logitsæ•°æ®
        if "logits" not in data.batch.keys() or data.batch["logits"] is None:
            print("âš ï¸  [HVRè­¦å‘Š] æœªæ‰¾åˆ°logitsæ•°æ®æˆ–logitsä¸ºNoneï¼Œå›é€€åˆ°åŸå§‹LogicRL")
            return super().__call__(data, return_dict)

        # å¦‚æœå·²æœ‰rm_scoresï¼Œç›´æ¥è¿”å›
        if "rm_scores" in data.batch.keys():
            if return_dict:
                return {"reward_tensor": data.batch["rm_scores"], "reward_extra_info": {}}
            else:
                return data.batch["rm_scores"]

        print("ğŸ¯ [HVR] å¼€å§‹è®¡ç®—æ··åˆä»·å€¼é‡å¡‘å¥–åŠ±...")

        # åˆå§‹åŒ–
        reward_tensor = torch.zeros_like(data.batch["responses"], dtype=torch.float32)
        all_hvr_metrics = {}

        # è·å–åŸºç¡€æ•°æ®
        logits = data.batch["logits"]  # (batch_size, seq_len, vocab_size)
        responses = data.batch["responses"]  # (batch_size, seq_len)

        # æŒ‰UIDåˆ†ç»„å¤„ç†
        uid_to_indices = {}
        for i, uid in enumerate(data.non_tensor_batch["uid"]):
            if uid not in uid_to_indices:
                uid_to_indices[uid] = []
            uid_to_indices[uid].append(i)

        for uid, indices in uid_to_indices.items():
            group_data = []

            for idx in indices:
                # è·å–responseæ–‡æœ¬
                response_ids = responses[idx]
                response_str = self.tokenizer.decode(response_ids, skip_special_tokens=True)

                # è®¡ç®—å¤–éƒ¨å¥–åŠ± (å¤ç”¨LogicRLé€»è¾‘)
                data_source = data.non_tensor_batch.get(self.reward_fn_key, ["unknown"])[idx]
                ground_truth = data.non_tensor_batch.get("ground_truth", [""])[idx]

                compute_score_fn = _select_rm_score_fn(data_source)
                external_score = compute_score_fn(response_str, ground_truth)

                # å‡†å¤‡HVRæ•°æ®
                response_logits = logits[idx]  # (seq_len, vocab_size)
                group_data.append({
                    'logits': response_logits,
                    'ids': response_ids,
                    'r_final': external_score,
                    'external_score': external_score,
                    'index': idx
                })

            # åº”ç”¨HVRç®—æ³•
            hvr_returns, hvr_metrics = calculate_hvr_rewards_for_group(
                group_data, self.alpha, self.beta, self.lambda_hvr,
                self.use_zscore, self.target_scale
            )

            # å°†HVRå¥–åŠ±åˆ†é…åˆ°tokençº§åˆ«
            for i, (data_item, hvr_return) in enumerate(zip(group_data, hvr_returns)):
                idx = data_item['index']
                # å°†åºåˆ—çº§å¥–åŠ±å¤åˆ¶åˆ°æ‰€æœ‰token (ä¿æŒä¸åŸLogicRLä¸€è‡´)
                reward_tensor[idx, :] = hvr_return

            # æ”¶é›†æŒ‡æ ‡
            for key, value in hvr_metrics.items():
                if key not in all_hvr_metrics:
                    all_hvr_metrics[key] = []
                all_hvr_metrics[key].append(value)

        # æ±‡æ€»æ‰€æœ‰æŒ‡æ ‡
        final_metrics = {}
        for key, values in all_hvr_metrics.items():
            if values:
                final_metrics[f"rewards/{key}"] = np.mean(values)

        # æ‰“å°å…³é”®æŒ‡æ ‡
        if final_metrics:
            print(f"ğŸ¯ [HVRæŒ‡æ ‡] V_ervf_mean: {final_metrics.get('rewards/v_ervf_mean', 0):.4f}")
            print(f"ğŸ¯ [HVRæŒ‡æ ‡] V_target_mean: {final_metrics.get('rewards/v_target_mean', 0):.4f}")
            print(f"ğŸ¯ [HVRæŒ‡æ ‡] final_return_mean: {final_metrics.get('rewards/final_return_mean', 0):.4f}")
            print(f"ğŸ¯ [HVRæŒ‡æ ‡] external_score_mean: {final_metrics.get('rewards/external_score_mean', 0):.4f}")

        if return_dict:
            return {
                "reward_tensor": reward_tensor,
                "reward_extra_info": final_metrics
            }
        else:
            return reward_tensor
