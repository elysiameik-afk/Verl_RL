#!/usr/bin/env python3
"""
æ­£ç¡®çš„Tokençº§EMAå®ç°
"""

import torch
import numpy as np

def apply_token_level_ema_smoothing(
    raw_weights: torch.Tensor,
    response_mask: torch.Tensor,
    beta: float = 0.9,
) -> tuple[torch.Tensor, dict]:
    """
    Apply token-level EMA smoothing within each sequence.
    
    æ ¸å¿ƒåˆ›æ–°ï¼šå¯¹åŒä¸€åºåˆ—å†…çš„tokenæƒé‡è¿›è¡Œæ—¶åºå¹³æ»‘
    w'[i,t] = Î² Ã— w[i,t] + (1-Î²) Ã— w'[i,t-1]
    
    Args:
        raw_weights: [batch_size, seq_len] - åŸå§‹é‡è¦æ€§æƒé‡ w[i,t]
        response_mask: [batch_size, seq_len] - æœ‰æ•ˆtokençš„mask
        beta: float - EMAå¹³æ»‘å› å­ (0 < Î² â‰¤ 1)
    
    Returns:
        tuple: (smoothed_weights, ema_metrics)
    """
    batch_size, seq_len = raw_weights.shape
    smoothed_weights = torch.zeros_like(raw_weights)
    
    sequence_variance_reductions = []
    sequence_smoothing_effects = []
    
    # å¯¹æ¯ä¸ªåºåˆ—åº”ç”¨tokençº§EMA
    for i in range(batch_size):
        sequence_mask = response_mask[i]  # [seq_len]
        sequence_weights = raw_weights[i]  # [seq_len]
        smoothed_sequence = torch.zeros_like(sequence_weights)
        
        # æ‰¾åˆ°æœ‰æ•ˆtokenä½ç½®
        valid_positions = torch.where(sequence_mask > 0)[0]
        if len(valid_positions) == 0:
            smoothed_weights[i] = sequence_weights
            continue
            
        # åˆå§‹åŒ–ç¬¬ä¸€ä¸ªæœ‰æ•ˆtoken: w'[i,0] = w[i,0]
        first_pos = valid_positions[0].item()
        smoothed_sequence[first_pos] = sequence_weights[first_pos]
        
        # å¯¹åç»­æœ‰æ•ˆtokenåº”ç”¨EMA: w'[i,t] = Î² Ã— w[i,t] + (1-Î²) Ã— w'[i,t-1]
        prev_smoothed = smoothed_sequence[first_pos]
        for pos_idx in range(1, len(valid_positions)):
            t = valid_positions[pos_idx].item()
            current_raw = sequence_weights[t]
            current_smoothed = beta * current_raw + (1 - beta) * prev_smoothed
            smoothed_sequence[t] = current_smoothed
            prev_smoothed = current_smoothed
        
        # å¤åˆ¶æ— æ•ˆtoken
        invalid_mask = sequence_mask == 0
        smoothed_sequence[invalid_mask] = sequence_weights[invalid_mask]
        
        smoothed_weights[i] = smoothed_sequence
        
        # è®¡ç®—æ¯ä¸ªåºåˆ—çš„æŒ‡æ ‡
        if len(valid_positions) > 1:
            valid_raw = sequence_weights[valid_positions]
            valid_smoothed = smoothed_sequence[valid_positions]
            
            raw_var = valid_raw.var()
            smoothed_var = valid_smoothed.var()
            
            if raw_var > 1e-8:
                var_reduction = raw_var / (smoothed_var + 1e-8)
                sequence_variance_reductions.append(var_reduction.item())
            
            smoothing_effect = torch.norm(valid_raw - valid_smoothed).item()
            sequence_smoothing_effects.append(smoothing_effect)
    
    # è®¡ç®—æ•´ä½“æŒ‡æ ‡
    raw_variance = (raw_weights * response_mask).var()
    smoothed_variance = (smoothed_weights * response_mask).var()
    overall_variance_reduction = raw_variance / (smoothed_variance + 1e-8)
    
    # ç¼–è¯‘æœ€ç»ˆæŒ‡æ ‡
    ema_metrics = {
        # æ ¸å¿ƒæ–¹å·®æŒ‡æ ‡
        'ema/raw_weights_variance': raw_variance.item(),
        'ema/smoothed_weights_variance': smoothed_variance.item(),
        'ema/variance_reduction_ratio': overall_variance_reduction.item(),
        
        # å¹³æ»‘æ•ˆæœæŒ‡æ ‡
        'ema/avg_sequence_variance_reduction': np.mean(sequence_variance_reductions) if sequence_variance_reductions else 1.0,
        'ema/avg_smoothing_effect': np.mean(sequence_smoothing_effects) if sequence_smoothing_effects else 0.0,
        
        # åŸºç¡€ç»Ÿè®¡
        'ema/raw_weights_mean': (raw_weights * response_mask).mean().item(),
        'ema/smoothed_weights_mean': (smoothed_weights * response_mask).mean().item(),
        'ema/raw_weights_std': (raw_weights * response_mask).std().item(),
        'ema/smoothed_weights_std': (smoothed_weights * response_mask).std().item(),
        
        # é…ç½®ä¿¡æ¯
        'ema/beta': beta,
        'ema/use_ema': True,
        'ema/processed_sequences': batch_size,
        'ema/total_valid_tokens': response_mask.sum().item(),
    }
    
    # è°ƒè¯•è¾“å‡º
    if torch.distributed.get_rank() == 0 and batch_size > 0:
        i = 0
        valid_mask = response_mask[i] > 0
        if valid_mask.sum() > 1:
            raw_seq = raw_weights[i][valid_mask]
            smoothed_seq = smoothed_weights[i][valid_mask]
            print(f"ğŸ” [TOKEN-EMA] åºåˆ—{i} (æœ‰æ•ˆtokenæ•°: {valid_mask.sum().item()}):")
            print(f"  åŸå§‹æƒé‡å‰5ä¸ª: {raw_seq[:5].tolist()}")
            print(f"  å¹³æ»‘æƒé‡å‰5ä¸ª: {smoothed_seq[:5].tolist()}")
            print(f"  æ–¹å·®å˜åŒ–: {raw_seq.var().item():.6f} â†’ {smoothed_seq.var().item():.6f}")
            print(f"  å¹³æ»‘å¼ºåº¦: {torch.norm(raw_seq - smoothed_seq).item():.6f}")
    
    return smoothed_weights, ema_metrics

def test_token_ema():
    """æµ‹è¯•tokençº§EMA"""
    print("ğŸ§ª æµ‹è¯•Tokençº§EMA...")
    
    # åˆ›å»ºæœ‰å˜åŒ–çš„æƒé‡åºåˆ—
    batch_size, seq_len = 2, 6
    raw_weights = torch.tensor([
        [1.5, 0.8, 1.2, 0.9, 1.1, 0.7],  # åºåˆ—1ï¼šæœ‰æ˜æ˜¾å˜åŒ–
        [1.0, 1.3, 0.6, 1.4, 0.8, 1.2],  # åºåˆ—2ï¼šæœ‰æ˜æ˜¾å˜åŒ–
    ])
    response_mask = torch.ones(batch_size, seq_len)
    beta = 0.7  # è¾ƒä½çš„betaä»¥çœ‹åˆ°æ›´æ˜æ˜¾æ•ˆæœ
    
    print(f"åŸå§‹æƒé‡:")
    print(f"  åºåˆ—1: {raw_weights[0].tolist()}")
    print(f"  åºåˆ—2: {raw_weights[1].tolist()}")
    print(f"  åºåˆ—1æ–¹å·®: {raw_weights[0].var().item():.6f}")
    print(f"  åºåˆ—2æ–¹å·®: {raw_weights[1].var().item():.6f}")
    
    smoothed_weights, metrics = apply_token_level_ema_smoothing(
        raw_weights=raw_weights,
        response_mask=response_mask,
        beta=beta,
    )
    
    print(f"\nå¹³æ»‘åæƒé‡:")
    print(f"  åºåˆ—1: {smoothed_weights[0].tolist()}")
    print(f"  åºåˆ—2: {smoothed_weights[1].tolist()}")
    print(f"  åºåˆ—1æ–¹å·®: {smoothed_weights[0].var().item():.6f}")
    print(f"  åºåˆ—2æ–¹å·®: {smoothed_weights[1].var().item():.6f}")
    
    print(f"\nEMAæŒ‡æ ‡:")
    for key, value in metrics.items():
        if isinstance(value, float):
            print(f"  {key}: {value:.6f}")
        else:
            print(f"  {key}: {value}")
    
    return smoothed_weights, metrics

if __name__ == "__main__":
    test_token_ema()
