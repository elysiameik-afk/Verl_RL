创新点一：截断平均重要性校正 (Trimmed Mean Importance Correction, TMIC)
1. 解决什么问题
TMIC旨在解决GSPO和AMIC（算术平均）中序列级重要性权重 s_i 因个别词元权重 w_{i,t} 的极端值（过大或过小）而产生剧烈波动的问题。这种波动会引入训练噪音，导致策略更新不稳定。
2. 如何解决的
通过在计算序列级平均权重之前，系统性地移除掉当前序列中最高和最低的一小部分词元级权重值，然后对剩余的“中间”权重进行平均。这可以显著增强序列级权重对异常值的鲁棒性。
3. 原理是什么
其原理是稳健统计学 (Robust Statistics) 中的截断平均 (Trimmed Mean)。截断平均是一种比普通算术平均更能抵抗离群点（outliers）影响的中心趋势估计方法。通过忽略数据分布两端的极端值，它能提供一个更稳定、更可信的样本均值估计，从而降低最终梯度的方差。
4. 具体实现细节
实现位置:
此模块完全替换GSPO算法中计算序列级重要性权重 s_i 的步骤。它发生在计算完一个序列中所有词元的原始重要性权重 w_{i,t} 之后，但在将序列级权重 s_i 应用于PPO的裁剪目标函数之前。
输入:
词元权重列表 W_i: 对于一个给定的序列 y_i，这是一个包含其所有词元原始重要性权重的列表。列表形式为 [w_{i,1}, w_{i,2}, ..., w_{i,L}]，其中 L 是序列长度 |y_i|，w_{i,t} = π_θ(y_{i,t}|...) / π_{θ_old}(y_{i,t}|...)。
截断比例 α (alpha): 这是一个浮点数超参数，范围在 [0, 0.5) 之间，通常设置为一个较小的值，如 0.1。它代表需要从列表的两端各自移除的数据比例。
计算步骤:
排序: 将输入的词元权重列表 W_i 从小到大进行排序。
计算截断数量: 根据序列长度 L 和截断比例 α，计算需要从列表的头部和尾部各移除的元素数量 k。计算公式为 k = floor(α * L)，其中 floor 是向下取整函数。
截断列表: 从已排序的列表中移除最小的 k 个元素和最大的 k 个元素，得到一个新的、更短的权重列表 W'_i。新列表的长度为 L - 2k。
计算平均值: 计算新列表 W'_i 中所有元素的算术平均值。这个平均值就是最终的TMIC序列级权重 s_i。
输出:
一个单一的浮点数值 s_i，代表该序列经过TMIC校正后的重要性权重。
后续使用:
将这个输出的 s_i 值，代入GSPO标准的目标函数中，用于后续的裁剪（clipping）和梯度加权，即 min(s_i * Â_i, clip(s_i, 1-ε, 1+ε) * Â_i)。
创新点二：基于惊奇度的信用分配 (Surprisal-Based Credit Assignment, SBCA)
1. 解决什么问题
SBCA旨在解决GSPO中“均匀信用分配”的低效率问题。GSPO将序列的总奖励/优势 Â_i 平均地分配给序列中的每一个词元，这忽略了不同词元对最终结果贡献度的巨大差异，导致学习信号被稀释。
2. 如何解决的
SBCA引入一个与每个词元 y_{i,t} 的“信息量”或“惊奇度”成正比的动态权重 c_{i,t}。它将总的奖励/优势 Â_i 按照这个权重进行非均匀地重新分配，使得那些模型认为更“出乎意料”（通常也是更关键）的词元获得更多的信用。
3. 原理是什么
其原理是信息论 (Information Theory)。一个事件的自信息 (Self-Information) 或惊奇度 (Surprisal) 定义为其概率的负对数 I(x) = -log(P(x))。一个低概率事件的发生，携带着更多的信息。SBCA假设，在一个成功的推理序列中，贡献最大的步骤正是那些信息量最大的步骤。通过将信用与信息量对齐，可以加速模型在关键决策点上的学习。
4. 具体实现细节
实现位置:
此模块作用于计算最终策略梯度的环节。它在计算出序列级权重 s_i 和序列级优势 Â_i 之后，但在将它们应用于每个词元的对数概率梯度 ∇log π_θ(y_{i,t}|...) 之前。它为每个词元的梯度项引入了一个额外的乘数。
输入:
词元对数概率列表 LogP_i: 对于一个给定的序列 y_i，这是一个包含其所有词元在当前策略 π_θ 下的对数概率的列表。列表形式为 [log π_θ(y_{i,1}|...), log π_θ(y_{i,2}|...), ..., log π_θ(y_{i,L}|...)]。这些值在模型的前向传播和损失计算中已经可用，无需额外计算。
计算步骤:
计算惊奇度: 将输入的词元对数概率列表 LogP_i 中的每一个元素取负，得到一个惊奇度列表 U_i。即 u_{i,t} = -log π_θ(y_{i,t}|...)。
归一化权重: 对惊奇度列表 U_i 应用 Softmax 函数。这将把惊奇度值转换成一个和为1的概率分布 p_{i,t}。
调整尺度: 将Softmax的输出 p_{i,t} 乘以序列的长度 L，得到最终的贡献权重列表 C_i。即 c_{i,t} = softmax_t(U_i) * L。乘以 L 是为了让权重的平均值保持为1，从而维持整体梯度更新的尺度与原始GSPO一致，避免引入不必要的超参数调整。
输出:
一个与输入序列等长的贡献权重列表 C_i，形式为 [c_{i,1}, c_{i,2}, ..., c_{i,L}]，其中每个元素 c_{i,t} 是一个正的浮点数。
后续使用:
在计算序列 y_i 对总损失的梯度贡献时，对于序列中的第 t 个词元，其梯度项不再是 s_i * Â_i * ∇log π_θ(y_{i,t}|...)，而是被修改为 s_i * Â_i * c_{i,t} * ∇log π_θ(y_{i,t}|...)。即，每个词元的梯度在被序列级权重和优势加权之前，先被其自身的贡献权重 c_{i,t} 进行了缩放。








