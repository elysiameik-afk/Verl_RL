1.我们知道少量的高质量样本对于强化学习的训练来说其实比大量低质量的数据更好。所以我们可以提前对
样本进行采样，选取方差高的少量样本进行RL，从数学上理解是，策略梯度和logπ（a|s）以及R（s，a）有关，方差大实际上是在对期望 E 进行采样时，改变了样本的构成，从而直接影响了公式中的 R(s,a) 项的分布，提高了学习效率和梯度估计的质量

2.多源奖励函数
r=最终的可验证得分+<think><answer>这种格式正确token(前两项来自KK_compute_score)+内部的势函数(logits-均匀分布，得到的是自信程度，好的模型应该越来越自信)，
解决奖励稀疏问题

解耦，放开置信域区间

3.信用分配
GRPO还有个大问题在于奖励分配平均化，奖励细致化避免稀疏，我们认为高概率的token不太需要被梯度更新，已经学的很好了，所以使用掩码，导致回答路径不同的应该是一些疑惑度较高的token，所以这样做到了奖励分配更精确。同时内部的势函数(logits-均匀分布，得到的是自信程度，好的模型应该越来越自信)这样的分段计算，也导致奖励分配更加细致了。

4.添加<think>token，<answer>token，<verify>token，cot有助于提高模型的推理上限，同时要对cot的长度进行惩罚，不能过长也不能过短，这是因为如果有适量的推理过程，奖励才能更好的分配到具体位置，模型才知道回答的好坏。

5.信噪分离 (SNS): 在轨迹层面，只相信模型最“斩钉截铁”的判断（最好和最差的），过滤掉噪声。生成的多条轨迹里只要最后的分数高的，和低的，这样模型更知道什么是好和坏，当然了要仔细和内部的势函数这样的分段奖励结合

6.高偏度和低散度的loss。