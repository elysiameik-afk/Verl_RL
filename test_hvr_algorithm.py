#!/usr/bin/env python3
"""
æµ‹è¯•HVR (Hindsight Value Reshaping) å†…ç”Ÿå¥–åŠ±ç®—æ³•
"""

import torch
import numpy as np
from verl.trainer.ppo.core_algos import calculate_ervf_value, calculate_hvr_rewards, apply_hvr_integration

def test_ervf_value():
    """æµ‹è¯•ERVFä»·å€¼å‡½æ•°è®¡ç®—"""
    print("ğŸ”¬ æµ‹è¯•ERVFä»·å€¼å‡½æ•°è®¡ç®—\n")
    
    # åˆ›å»ºæµ‹è¯•logits
    vocab_size = 1000
    logits = torch.randn(vocab_size)
    
    # æµ‹è¯•ä¸åŒå‚æ•°
    test_cases = [
        (1.0, 0.0),   # æ— ç†µæƒ©ç½š
        (1.0, 0.1),   # è½»å¾®ç†µæƒ©ç½š
        (1.0, 0.5),   # ä¸­ç­‰ç†µæƒ©ç½š
        (0.5, 0.1),   # ä½æ¸©åº¦
        (2.0, 0.1),   # é«˜æ¸©åº¦
    ]
    
    for alpha, beta in test_cases:
        v_ervf = calculate_ervf_value(logits, alpha, beta)
        
        # æ‰‹åŠ¨è®¡ç®—éªŒè¯
        v_endo = alpha * torch.logsumexp(logits / alpha, dim=0).item()
        probs = torch.softmax(logits, dim=0)
        entropy = -torch.sum(probs * torch.log(probs + 1e-8)).item()
        expected_v_ervf = v_endo - beta * entropy
        
        print(f"Î±={alpha}, Î²={beta}:")
        print(f"  V_ERVF: {v_ervf:.4f}")
        print(f"  V_endo: {v_endo:.4f}")
        print(f"  ç†µ: {entropy:.4f}")
        print(f"  é¢„æœŸå€¼: {expected_v_ervf:.4f}")
        print(f"  åŒ¹é…: {'âœ…' if abs(v_ervf - expected_v_ervf) < 1e-6 else 'âŒ'}")
        print()

def test_hvr_rewards():
    """æµ‹è¯•HVRå¥–åŠ±è®¡ç®—"""
    print("ğŸ¯ æµ‹è¯•HVRå¥–åŠ±è®¡ç®—\n")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    seq_len = 5
    vocab_size = 100
    
    # éšæœºlogitså’Œtokenåºåˆ—
    response_logits = torch.randn(seq_len, vocab_size)
    response_ids = torch.randint(0, vocab_size, (seq_len,))
    
    # æµ‹è¯•ä¸åŒçš„R_finalå€¼
    r_final_values = [-3.0, -1.0, 0.0, 1.0, 3.0]
    
    for r_final in r_final_values:
        print(f"ğŸ“Š R_final = {r_final}")
        
        hvr_rewards = calculate_hvr_rewards(
            response_logits=response_logits,
            response_ids=response_ids,
            R_final=r_final,
            alpha=1.0,
            beta=0.1,
            lambda_hvr=0.5,
        )
        
        print(f"  HVRå¥–åŠ±: {hvr_rewards.tolist()}")
        print(f"  å¥–åŠ±æ€»å’Œ: {hvr_rewards.sum().item():.4f}")
        print(f"  å¥–åŠ±å‡å€¼: {hvr_rewards.mean().item():.4f}")
        print(f"  æœ€åå¥–åŠ±: {hvr_rewards[-1].item():.4f}")
        print()

def test_hvr_integration():
    """æµ‹è¯•HVRé›†æˆåŠŸèƒ½"""
    print("ğŸ”§ æµ‹è¯•HVRé›†æˆåŠŸèƒ½\n")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 2
    seq_len = 10
    vocab_size = 50
    
    # åŸå§‹advantages
    advantages = torch.randn(batch_size, seq_len)
    
    # å“åº”logitså’ŒIDs
    response_logits = torch.randn(batch_size, seq_len, vocab_size)
    response_ids = torch.randint(0, vocab_size, (batch_size, seq_len))
    
    # å“åº”mask (æ¨¡æ‹Ÿä¸åŒé•¿åº¦çš„åºåˆ—)
    response_mask = torch.ones(batch_size, seq_len)
    response_mask[0, 8:] = 0  # ç¬¬ä¸€ä¸ªåºåˆ—é•¿åº¦ä¸º8
    response_mask[1, 6:] = 0  # ç¬¬äºŒä¸ªåºåˆ—é•¿åº¦ä¸º6
    
    print("åŸå§‹advantages:")
    for i in range(batch_size):
        valid_pos = torch.where(response_mask[i] > 0)[0]
        print(f"  åºåˆ—{i}: {advantages[i, valid_pos].tolist()}")
    
    # åº”ç”¨HVR
    enhanced_advantages, hvr_metrics = apply_hvr_integration(
        advantages=advantages,
        response_logits=response_logits,
        response_ids=response_ids,
        response_mask=response_mask,
        alpha=1.0,
        beta=0.1,
        lambda_hvr=0.5,
    )
    
    print("\nHVRå¢å¼ºåçš„advantages:")
    for i in range(batch_size):
        valid_pos = torch.where(response_mask[i] > 0)[0]
        print(f"  åºåˆ—{i}: {enhanced_advantages[i, valid_pos].tolist()}")
    
    print("\nHVRæŒ‡æ ‡:")
    for key, value in hvr_metrics.items():
        print(f"  {key}: {value}")

def test_parameter_sensitivity():
    """æµ‹è¯•å‚æ•°æ•æ„Ÿæ€§"""
    print("ğŸ›ï¸ æµ‹è¯•å‚æ•°æ•æ„Ÿæ€§\n")
    
    # å›ºå®šæµ‹è¯•æ•°æ®
    seq_len = 8
    vocab_size = 100
    response_logits = torch.randn(seq_len, vocab_size)
    response_ids = torch.randint(0, vocab_size, (seq_len,))
    r_final = 1.0
    
    # æµ‹è¯•alphaçš„å½±å“
    print("ğŸ“ˆ Alphaå‚æ•°å½±å“:")
    alphas = [0.5, 1.0, 2.0, 4.0]
    for alpha in alphas:
        hvr_rewards = calculate_hvr_rewards(
            response_logits, response_ids, r_final,
            alpha=alpha, beta=0.1, lambda_hvr=0.5
        )
        print(f"  Î±={alpha}: å‡å€¼={hvr_rewards.mean().item():.4f}, æ ‡å‡†å·®={hvr_rewards.std().item():.4f}")
    
    print("\nğŸ“ˆ Betaå‚æ•°å½±å“:")
    betas = [0.0, 0.1, 0.3, 0.5]
    for beta in betas:
        hvr_rewards = calculate_hvr_rewards(
            response_logits, response_ids, r_final,
            alpha=1.0, beta=beta, lambda_hvr=0.5
        )
        print(f"  Î²={beta}: å‡å€¼={hvr_rewards.mean().item():.4f}, æ ‡å‡†å·®={hvr_rewards.std().item():.4f}")
    
    print("\nğŸ“ˆ Lambdaå‚æ•°å½±å“:")
    lambdas = [0.0, 0.3, 0.5, 0.7, 1.0]
    for lambda_hvr in lambdas:
        hvr_rewards = calculate_hvr_rewards(
            response_logits, response_ids, r_final,
            alpha=1.0, beta=0.1, lambda_hvr=lambda_hvr
        )
        print(f"  Î»={lambda_hvr}: å‡å€¼={hvr_rewards.mean().item():.4f}, æ ‡å‡†å·®={hvr_rewards.std().item():.4f}")

def test_edge_cases():
    """æµ‹è¯•è¾¹ç•Œæƒ…å†µ"""
    print("ğŸ” æµ‹è¯•è¾¹ç•Œæƒ…å†µ\n")
    
    # æµ‹è¯•æçŸ­åºåˆ—
    print("ğŸ“ æçŸ­åºåˆ— (é•¿åº¦=1):")
    short_logits = torch.randn(1, 100)
    short_ids = torch.randint(0, 100, (1,))
    short_rewards = calculate_hvr_rewards(short_logits, short_ids, 1.0)
    print(f"  å¥–åŠ±: {short_rewards.tolist()}")
    
    # æµ‹è¯•æç«¯R_finalå€¼
    print("\nğŸ“Š æç«¯R_finalå€¼:")
    normal_logits = torch.randn(3, 100)
    normal_ids = torch.randint(0, 100, (3,))
    
    extreme_r_finals = [-3.0, 3.0]
    for r_final in extreme_r_finals:
        rewards = calculate_hvr_rewards(normal_logits, normal_ids, r_final)
        print(f"  R_final={r_final}: {rewards.tolist()}")
    
    # æµ‹è¯•æ•°å€¼ç¨³å®šæ€§
    print("\nğŸ”¢ æ•°å€¼ç¨³å®šæ€§æµ‹è¯•:")
    # åˆ›å»ºæå¤§çš„logits
    large_logits = torch.randn(3, 100) * 10
    large_rewards = calculate_hvr_rewards(large_logits, normal_ids, 0.0)
    print(f"  å¤§logits: æ˜¯å¦æœ‰NaN={torch.isnan(large_rewards).any().item()}")
    print(f"  å¤§logits: æ˜¯å¦æœ‰Inf={torch.isinf(large_rewards).any().item()}")

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æµ‹è¯•HVR (Hindsight Value Reshaping) å†…ç”Ÿå¥–åŠ±ç®—æ³•\n")
    
    test_ervf_value()
    test_hvr_rewards()
    test_hvr_integration()
    test_parameter_sensitivity()
    test_edge_cases()
    
    print("ğŸ‰ HVRç®—æ³•æµ‹è¯•å®Œæˆï¼")
    print("\nğŸ“‹ HVRç®—æ³•ç‰¹æ€§æ€»ç»“:")
    print("  âœ… ERVFä»·å€¼å‡½æ•°: ç»“åˆEndoRMå’Œç†µæ­£åˆ™åŒ–")
    print("  âœ… ç¨ å¯†å¥–åŠ±ç”Ÿæˆ: åŸºäºä»·å€¼è½¨è¿¹é‡å¡‘")
    print("  âœ… ç¨€ç–å¥–åŠ±é›†æˆ: R_finalæŒ‡å¯¼ä»·å€¼ç›®æ ‡")
    print("  âœ… å‚æ•°å¯æ§: Î±æ§åˆ¶æ¸©åº¦ï¼ŒÎ²æ§åˆ¶ç†µæƒ©ç½šï¼ŒÎ»æ§åˆ¶æ··åˆ")
    print("  âœ… æ•°å€¼ç¨³å®š: ä½¿ç”¨log_softmaxé¿å…æ•°å€¼é—®é¢˜")
    print("\nğŸ¯ HVRå†…ç”Ÿå¥–åŠ±æœºåˆ¶å·²å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒï¼")
